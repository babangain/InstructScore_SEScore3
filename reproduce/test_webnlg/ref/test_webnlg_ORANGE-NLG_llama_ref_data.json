{"type": "text2score", "instances": [{"input": "You are evaluating RDF-to-text task. The correct generation is \"Aaron Turner starter performing in 1995 and began singing for a living. He performs under the genre of post-metal, and is noted to have played with the band Twilight.\". The input of model is \"['Aaron_Turner | genre | Post-metal', 'Aaron_Turner | instrument | Singing', 'Aaron_Turner | associatedBand/associatedMusicalArtist | Twilight_(band)', 'Aaron_Turner | activeYearsStartYear | 1995']\". The model generated output is \"Aaron Turner is a post-metal singer who started performing in 1995. He played with the band Twilight.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 99.66666666666667, "Fluency": 95.33333333333333, "Relevance": 100.0, "TextStructure": 100.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nurhan Atasoy, who was born on January 01, 1934 in Turkey, currently also resides there (Istanbul) and has both Turkish nationality and citizenship.\". The input of model is \"['Nurhan_Atasoy | birthDate | 1934-01-01', 'Nurhan_Atasoy | residence | Istanbul', 'Nurhan_Atasoy | citizenship | Turkey', 'Nurhan_Atasoy | birthPlace | Turkey', 'Nurhan_Atasoy | nationality | Turkish_people', 'Nurhan_Atasoy | residence | Turkey']\". The model generated output is \"The Turkish people are the people who live in Turkey and are the residence of Nur-speed Atasoy. Their birth date is 1934-01-01 and they reside in Istanbul. Their citizenship is Turkey and they reside in Turkey.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 35.666666666666664, "DataCoverage": 61.0, "Fluency": 37.666666666666664, "Relevance": 57.0, "TextStructure": 38.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Bootleg Series Volume 1: The Quine Tapes was released by Universal Music Group, a record label run by key person Lucian Grainge. Polydor Records recorded Bootleg Series Volume 1: The Quine Tapes.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Polydor_Records', 'Universal_Music_Group | keyPerson | Lucian_Grainge', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Universal_Music_Group']\". The model generated output is \"Lucian Grainge is a key person at Universal Music group who signed with Polydor Records and formerly with The Universal Music Group. Their record label is Universal Music Group.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 49.5, "DataCoverage": 46.0, "Fluency": 49.0, "Relevance": 69.5, "TextStructure": 52.5}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Agremiacao Sportiva Arapiraquense club has 17000 members and they play in the Campeonato Brasileiro S\u00e9rie C league based in Brazil. Their ground is the Estadio Municipal Coaracy da Mata Fonseca located in Arapiraca and Vila Nova Futebol Clube are the champions of the league. The chairman of Agremia\u00e7\u00e3o Sportiva Arapiraquense has the title of 'President'.\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Campeonato_Brasileiro_S\u00e9rie_C | champions | Vila_Nova_Futebol_Clube', 'Campeonato_Brasileiro_S\u00e9rie_C | country | Brazil', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | numberOfMembers | 17000', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | chairmanTitle | \"President\"']\". The model generated output is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense's ground is the Estadio Municipal Coaracy da Mata Fonseca in Arapiraca, Brazil. They have 17000 members and play in the Campeonato Brasileiro S\u00e9rie C league where Vila Nova Futebol Clube have been champions. The title of the chairman of the Campeonato Brasileiro S\u00e9rie C is President and they have 17000 members.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 83.66666666666667, "DataCoverage": 99.33333333333333, "Fluency": 76.33333333333333, "Relevance": 98.66666666666667, "TextStructure": 91.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"English Without Tears producer Anatole de Grunwald was born in Saint Petersburg.\". The input of model is \"['English_Without_Tears | producer | Anatole_de_Grunwald', 'Anatole_de_Grunwald | birthPlace | Saint_Petersburg']\". The model generated output is \"Anatole de Grunwald, born in Saint Petersburg, is the manufacturer of English Fenchurch.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 81.0, "DataCoverage": 65.0, "Fluency": 96.33333333333333, "Relevance": 64.0, "TextStructure": 93.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The album Turn Me On is by the American band The Honeymoon Killers.\". The input of model is \"Turn_Me_On_(album) | artist | The_Honeymoon_Killers_(American_band)\". The model generated output is \"Turn Me On is an artist for The Honeoon Killers, an American band.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 29.333333333333332, "DataCoverage": 95.0, "Fluency": 37.666666666666664, "Relevance": 96.0, "TextStructure": 92.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Live at Roadburn 2008 by Year of No Light was produced by the Roadburn Festival.\". The input of model is \"Live_at_Roadburn_2008_(Year_of_No_Light_album) | producer | Roadburn_Festival\". The model generated output is \"Live at Roadburn 2008 was manufactured by Roadburn Festival.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 80.66666666666667, "DataCoverage": 80.66666666666667, "Fluency": 79.66666666666667, "Relevance": 75.66666666666667, "TextStructure": 83.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The film Super Capers, distributed by Lionsgate and starring Justin Whalin, grossed $30,955.\". The input of model is \"['Super_Capers | distributor | Lionsgate', 'Super_Capers | starring | Justin_Whalin', 'Super_Capers | gross | 30955.0']\". The model generated output is \"The distributor of the distributor of Super pedas is bakewell onsgate. It starred Justin Whalin and has 30955 gross.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 58.0, "DataCoverage": 89.0, "Fluency": 40.0, "Relevance": 93.0, "TextStructure": 42.5}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Bootleg Series Volume 1: The Quine Tapes by Velvet Underground, a rock music genre, was released on October, 16, 2001 by Universal Music Group and Polydor Records.The Quine Tapes was preceded by Squeeze by The Velvet Underground.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | artist | The_Velvet_Underground', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | genre | Rock_music', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | precededBy | Squeeze_(The_Velvet_Underground_album)', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Polydor_Records', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Universal_Music_Group', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | releaseDate | 2001-10-16']\". The model generated output is \"Located in Polydor Records, and with its Velvet Underground as well as being signed to the Universal Music group, and now signed to the record labels Universal Music Group, they develop rock music. Their musical genre is rock music and they have been preceded by Squeeze (The Velvet Underground).\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 90.5, "DataCoverage": 72.5, "Fluency": 86.0, "Relevance": 79.0, "TextStructure": 80.5}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The currency of Mexico is the peso and the local people are called Mexicans. In Mexico there is a region called Jalisco which makes a dessert called bionico that requires granola as one of its ingredients.\". The input of model is \"['Mexico | currency | Mexican_peso', 'Mexico | demonym | Mexicans', 'Bionico | ingredient | Granola', 'Bionico | region | Jalisco', 'Bionico | country | Mexico']\". The model generated output is \"Bionico is a food found in Jalisco, Mexico, where Mexican peso is the currency and Mexican people live. Granola is one of the ingredients.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 92.33333333333333, "DataCoverage": 95.0, "Fluency": 65.0, "Relevance": 96.33333333333333, "TextStructure": 74.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nie Haisheng, born in Zaoyang took part in the Shenzhou 6 mission was part of the mission Shenzhou 10.\". The input of model is \"['Nie_Haisheng | mission | Shenzhou_10', 'Nie_Haisheng | mission | Shenzhou_6', 'Nie_Haisheng | birthPlace | Zaoyang']\". The model generated output is \"Nie Haisheng was born in Zaoyang and was a crew member of Shenzhou 10. He was also a crew member of Shenzhou 6.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 91.0, "DataCoverage": 100.0, "Fluency": 72.0, "Relevance": 85.66666666666667, "TextStructure": 96.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The ALCO RS-3 was produced between May 1950 and August 1956 has has a cylinder count of 12 and is 17068.8 millimetres in length.\". The input of model is \"['ALCO_RS-3 | buildDate | \"May 1950 - August 1956\"', 'ALCO_RS-3 | cylinderCount | 12', 'ALCO_RS-3 | length | 17068.8 (millimetres)']\". The model generated output is \"The ALCO RS-3, produced between May 1950 and August 1956, has 12 cylinders and is 17068.8 (millimetres) in length.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 90.0, "Relevance": 100.0, "TextStructure": 95.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy is located in the city of Dijon, has 27400 students, 16800 undergraduate students, 2900 staff members and 1299 doctoral students.\". The input of model is \"['University_of_Burgundy | numberOfUndergraduateStudents | 16800', 'University_of_Burgundy | staff | 2900', 'University_of_Burgundy | numberOfStudents | 27400', 'University_of_Burgundy | numberOfDoctoralStudents | 1299', 'University_of_Burgundy | city | Dijon']\". The model generated output is \"The University of Burgundy in Dijon has 2900 staff and 27400 students. It has 16800 undergraduate students and 1299 doctoral students.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 96.66666666666667, "DataCoverage": 99.66666666666667, "Fluency": 97.33333333333333, "Relevance": 99.0, "TextStructure": 97.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Post-hardcore is a derivative of noise rock.\". The input of model is \"Noise_rock | derivative | Post-hardcore\". The model generated output is \"Noise rock has post-hardcore as a derivative.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.33333333333333, "DataCoverage": 100.0, "Fluency": 63.333333333333336, "Relevance": 99.66666666666667, "TextStructure": 86.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The All India Council for Technical Education was proud to give the Technical Campus status to the Acharya Institute of Technology located in Bangalore. The Institute is affiliated with Visvesvaraya Technological University.\". The input of model is \"['Acharya_Institute_of_Technology | affiliation | Visvesvaraya_Technological_University', 'Acharya_Institute_of_Technology | city | Bangalore', 'Acharya_Institute_of_Technology | wasGivenTheTechnicalCampusStatusBy | All_India_Council_for_Technical_Education']\". The model generated output is \"The Acharya Institute of Technology in Bangalore is affiliated with the Visvesvaraya Technological University and was given the technical campus status by the All India Council for Technical Education.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 91.0, "DataCoverage": 96.5, "Fluency": 96.0, "Relevance": 96.5, "TextStructure": 50.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The 11th Mississippi Infantry monument (located in Adams county, Pennsylvania) was established in the year 2000. Adams county, Pennsylvania has Cumberland county, Pennsylvania to its north.\". The input of model is \"['11th_Mississippi_Infantry_Monument | established | 2000', '11th_Mississippi_Infantry_Monument | location | Adams_County,_Pennsylvania', 'Adams_County,_Pennsylvania | hasToItsNorth | Cumberland_County,_Pennsylvania']\". The model generated output is \"The 11th Mississippi Infantry Monument was established in 2000 and is located in Adams County, Pennsylvania. To the north of Adams County lies Cumberland County.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 89.33333333333333, "DataCoverage": 91.33333333333333, "Fluency": 79.66666666666667, "Relevance": 89.33333333333333, "TextStructure": 79.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"J.R.R. Tolkien wrote The Fellowship of the Ring which was released on July 29, 1954. The Hobbit was released first, followed by The Fellowship of the Ring, and then The Two Towers.\". The input of model is \"['The_Fellowship_of_the_Ring | followedBy | The_Two_Towers', 'The_Fellowship_of_the_Ring | author | J._R._R._Tolkien', 'The_Fellowship_of_the_Ring | releaseDate | 1954-07-29', 'The_Fellowship_of_the_Ring | precededBy | The_Hobbit']\". The model generated output is \"The Fellowship of the Ring was written by J.R. Tolkien and followed by The Two Towers. It was preceded by The Hobbit and retired on July 9, 1954.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 79.33333333333333, "DataCoverage": 87.33333333333333, "Fluency": 76.66666666666667, "Relevance": 91.0, "TextStructure": 77.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The apoapsis of 1147 Stavropolis is 418476000000.0. Its orbital period of is 1249.6 days, and it was discovered by Grigory Neujmin.\". The input of model is \"['1147_Stavropolis | apoapsis | 418476000000.0', '1147_Stavropolis | orbitalPeriod | 1249.6', '1147_Stavropolis | discoverer | Grigory_Neujmin']\". The model generated output is \"Grigory Neujmin discovered the asteroid called asam pedas which has an orbital period of 1249.6 and an apoapsis of 418476000000.0.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 69.66666666666667, "DataCoverage": 85.33333333333333, "Fluency": 75.66666666666667, "Relevance": 78.0, "TextStructure": 74.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Brandon Carter was born in 1942 on January, 1 and graduated from the University of Cambridge. He is best known for creating the Penrose diagram.\". The input of model is \"['Brandon_Carter | knownFor | Penrose_diagram', 'Brandon_Carter | birthDate | 1942-01-01', 'Brandon_Carter | almaMater | University_of_Cambridge']\". The model generated output is \"Brandon Carter was born on 1942-01-01 and his alma mater was the University of Cambridge. He is known for Penrose diagram.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 88.0, "DataCoverage": 94.66666666666667, "Fluency": 78.66666666666667, "Relevance": 93.33333333333333, "TextStructure": 86.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"After releasing their album Squeeze, music group the Velvet Underground produced and released rock music album Bootleg Series Volume 1: The Quine Tapes, which they recorded in St Louis Missouri, USA The album was released on 2001-10-16 under the Polydor and Universal Music Group record labels and has a total run time of 230:05 minutes.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | recordedIn | United_States', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordedIn | St._Louis,_Missouri', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | precededBy | Squeeze_(The_Velvet_Underground_album)', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Polydor_Records', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Universal_Music_Group', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | releaseDate | 2001-10-16', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | runtime | 230.05']\". The model generated output is \"Located in the United States, and signed to the record labels Polydor Records and Universal Music Group, The record labels are Universal Music Group. It is signed to the record labels: St Louis, Bobina ouri, Squeeze (Velvet Underground) and retired on Aug 1, 2001.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 71.0, "DataCoverage": 73.0, "Fluency": 84.0, "Relevance": 67.33333333333333, "TextStructure": 86.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The song Mermaid is by the musical band Train.\". The input of model is \"Mermaid_(Train_song) | musicalBand | Train_(band)\". The model generated output is \"The musical genre of bakewell (part of the band) is bhangra.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 36.0, "DataCoverage": 24.333333333333332, "Fluency": 67.0, "Relevance": 17.0, "TextStructure": 70.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nurhan Atasoy's date of birth is January 1, 1934.\". The input of model is \"Nurhan_Atasoy | birthDate | 1934-01-01\". The model generated output is \"NurB\u00fct\u00fcner Atasoy was born on 1934-01-01.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 65.0, "DataCoverage": 100.0, "Fluency": 78.33333333333333, "Relevance": 84.0, "TextStructure": 91.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nord is an album by Year Of No Light that falls under the genres of sludge metal and post-metal. It was released in September 6th, 2006. The next album Year Of No Light released was Live at Roadburn 2008.\". The input of model is \"['Nord_(Year_of_No_Light_album) | releaseDate | 2006-09-06', 'Nord_(Year_of_No_Light_album) | followedBy | Live_at_Roadburn_2008_(Year_of_No_Light_album)', 'Nord_(Year_of_No_Light_album) | genre | Sludge_metal', 'Nord_(Year_of_No_Light_album) | genre | Post-metal']\". The model generated output is \"The released date of the asteroid called bakewell (which has the musical genre of sludge metal) is 2006-09-06. It was followed by Live at Roadburn 2008 and the musical genre post-metal.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 68.33333333333333, "DataCoverage": 89.66666666666667, "Fluency": 58.666666666666664, "Relevance": 76.66666666666667, "TextStructure": 72.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Ciudad Ayala is a city with population density of 1604.0 and the population of the metropolitan area is 1,777,539. One of the leaders of Ciudad Ayala is the City Manager.\". The input of model is \"['Ciudad_Ayala | populationMetro | 1777539', 'Ciudad_Ayala | leaderTitle | \"City Manager\"', 'Ciudad_Ayala | type | City', 'Ciudad_Ayala | populationDensity | 1604.0']\". The model generated output is \"The City Manager leads the city of Kuttikkattoor which is a city with a population density of 1604.0. It has a population density of 1777539.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 76.66666666666667, "DataCoverage": 74.0, "Fluency": 93.33333333333333, "Relevance": 98.0, "TextStructure": 89.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The album Nord by the artist Year of No Light, was released by the record label Crucial blast on the E-Vinyl record label.\". The input of model is \"['Nord_(Year_of_No_Light_album) | artist | Year_of_No_Light', 'Nord_(Year_of_No_Light_album) | recordLabel | Crucial_Blast', 'Nord_(Year_of_No_Light_album) | recordLabel | E-Vinyl']\". The model generated output is \"The record labels for bakewell are Crucial Blast and E-Vinyl. Their home is called the E-Vinyl and they were invented in the Year 2009.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 21.5, "DataCoverage": 33.0, "Fluency": 47.0, "Relevance": 25.0, "TextStructure": 47.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Peso is currency used by Mexicans in Mexico. Bionico is a dessert that can be found in Guadalajara, Mexico. Cookies are also a type of dessert.\". The input of model is \"['Mexico | currency | Mexican_peso', 'Mexico | demonym | Mexicans', 'Bionico | course | Dessert', 'Bionico | region | Guadalajara', 'Dessert | dishVariation | Cookie', 'Bionico | country | Mexico']\". The model generated output is \"Bionico is a dessert found in Guadalajara, Mexico, where Mexican peso is the currency and Mexican people live. Cookies are also a dessert.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 78.66666666666667, "DataCoverage": 81.66666666666667, "Fluency": 57.333333333333336, "Relevance": 90.66666666666667, "TextStructure": 66.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"La Crosse is a city in La Crosse County in the U.S. state of Wisconsin.\". The input of model is \"La_Crosse,_Wisconsin | isPartOf | La_Crosse_County,_Wisconsin\". The model generated output is \"La Crosse, Wisconsin is part of La Crosse County, Wisconsin.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 96.33333333333333, "Relevance": 100.0, "TextStructure": 95.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Lady Anne Monson was born in Darlington, in the Kingdom of England, on January 1st, 1726. Her nationality was with the Kingdom of England, but her residence was in India. She passed away on February 18th, 1776.\". The input of model is \"['Lady_Anne_Monson | birthPlace | Darlington', 'Lady_Anne_Monson | birthDate | 1726-01-01', 'Lady_Anne_Monson | deathDate | 1776-02-18', 'Lady_Anne_Monson | birthPlace | Kingdom_of_England', 'Lady_Anne_Monson | nationality | Kingdom_of_England', 'Lady_Anne_Monson | residence | India']\". The model generated output is \"Born in the Kingdom of England in 1726-01-01, and now resides in India, on the 18th of July, 1776. He retired on the 1st of July, 1776.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 35.333333333333336, "DataCoverage": 29.333333333333332, "Fluency": 48.666666666666664, "Relevance": 38.0, "TextStructure": 54.333333333333336}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Pontiac Rageous was a car with a coupe body style that was manufactured by Pontiac, first produced in 1997 and last produced in 1997 as well.\". The input of model is \"['Pontiac_Rageous | productionStartYear | 1997', 'Pontiac_Rageous | productionEndYear | 1997', 'Pontiac_Rageous | bodyStyle | Coupe', 'Pontiac_Rageous | manufacturer | Pontiac']\". The model generated output is \"Pontiac Rageous, manufactured in Pontiac, made in 1997, is a Coupe with its production start in 1997.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 82.66666666666667, "DataCoverage": 73.33333333333333, "Fluency": 78.0, "Relevance": 92.66666666666667, "TextStructure": 80.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nicholas Brodszky has a non-performing background.\". The input of model is \"Nicholas_Brodszky | background | non_performing_personnel\". The model generated output is \"Nicholas Brodszky started out as a non performing personnel.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.0, "DataCoverage": 98.66666666666667, "Fluency": 89.33333333333333, "Relevance": 97.33333333333333, "TextStructure": 88.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Cecil Parker had a main role in It's Great to Be Young.\". The input of model is \"It's_Great_to_Be_Young_(1956_film) | starring | Cecil_Parker\". The model generated output is \"Cecil Parker starred in the film version of Cecil Parker, which starred in the film's Great to.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 51.333333333333336, "DataCoverage": 60.333333333333336, "Fluency": 44.666666666666664, "Relevance": 62.0, "TextStructure": 49.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Darinka Dentcheva is a citizen of the United States who works in the field of Mathematical optimization.\". The input of model is \"['Darinka_Dentcheva | citizenship | United_States', 'Darinka_Dentcheva | professionalField | Mathematical_optimization']\". The model generated output is \"The United States citizenship known as bakinka Dentcheva is a professional field which offers services concitizenship.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 34.333333333333336, "DataCoverage": 96.0, "Fluency": 60.333333333333336, "Relevance": 79.0, "TextStructure": 83.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"While pop rock can trace its stylistic roots back to rock music, Reggae music evolved out of different musical genre, known as ska. Interestingly, the Train song, Mermaid, belongs to the genre of pop rock, but is also considered to be of the reggae genre as well.\". The input of model is \"['Mermaid_(Train_song) | genre | Pop_rock', 'Reggae | stylisticOrigin | Ska', 'Mermaid_(Train_song) | genre | Reggae', 'Pop_rock | stylisticOrigin | Rock_music']\". The model generated output is \"Reggae is a pop rock musician whose musical genre is Reggae. Rock music has its origins in Ska and rock music.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 80.66666666666667, "DataCoverage": 96.66666666666667, "Fluency": 85.33333333333333, "Relevance": 78.66666666666667, "TextStructure": 82.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The OS grid reference for Darlington is NZ289147.\". The input of model is \"Darlington | gridReference | NZ289147\". The model generated output is \"NZ289147 is the reference to bhajji lington.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 64.0, "DataCoverage": 34.0, "Fluency": 73.0, "Relevance": 35.5, "TextStructure": 85.5}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Aaron Turner is a singer who was born in Boston. His music is post-metal, and he played with the band twilight.\". The input of model is \"['Aaron_Turner | genre | Post-metal', 'Aaron_Turner | instrument | Singing', 'Aaron_Turner | associatedBand/associatedMusicalArtist | Twilight_(band)', 'Aaron_Turner | origin | Boston']\". The model generated output is \"Aaron Turner was born in Boston and is a singer in the Twilight band. His musical genre is post-metal.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 87.66666666666667, "DataCoverage": 89.0, "Fluency": 77.33333333333333, "Relevance": 83.33333333333333, "TextStructure": 84.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Aleksandr Prudnikov was born on 1989-02-24 and plays for FC Amkar Perm. He was also a member of the youth side of FC Spartak Moscow.\". The input of model is \"['Aleksandr_Prudnikov | club | FC_Amkar_Perm', 'Aleksandr_Prudnikov | birthDate | 1989-02-24', 'Aleksandr_Prudnikov | youthclub | FC_Spartak_Moscow', 'Aleksandr_Prudnikov | currentclub | FC_Amkar_Perm']\". The model generated output is \"Aleksandr Prudnikov was born on 24 February 1989 and plays for FC Amkar Perm and FC Spartak Moscow youth team. He currently plays for FC Amkar Perm.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 92.66666666666667, "DataCoverage": 92.33333333333333, "Fluency": 87.0, "Relevance": 97.0, "TextStructure": 84.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The song Mermaid by Train (written by Espen Lind) was release on December 27, 2012. Mermaid has a total runtime of 3 minutes and 16 seconds, and belongs to the pop rock genre. Mermaid is preceded by the song This'll be my year.\". The input of model is \"['Mermaid_(Train_song) | genre | Pop_rock', 'Mermaid_(Train_song) | runtime | 3.16', 'Mermaid_(Train_song) | releaseDate | 2012-12-27', \"Mermaid_(Train_song) | precededBy | This'll_Be_My_Year\", 'Mermaid_(Train_song) | writer | Espen_Lind']\". The model generated output is \"The musical genre of bakewell (19255) is pop rock and it was preceded by The'll 1089 My. It was created by Espen Lind and retired on 27 December 2012.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 40.333333333333336, "DataCoverage": 47.333333333333336, "Fluency": 53.0, "Relevance": 50.0, "TextStructure": 77.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Lady Anne Monson lived in the Kingdom of England and held British nationality but resided in India for most of her life. Pranab Mukherjee is the leader of India, which has a total area of 32875900000000.\". The input of model is \"['Lady_Anne_Monson | residence | India', 'India | leader | Pranab_Mukherjee', 'Lady_Anne_Monson | nationality | Kingdom_of_England', 'India | areaTotal | 3287590000000.0']\". The model generated output is \"Pranab Mukherjee is a leader in India which has a total area of 3287590000000.0. The country's nationality is Kingdom of England and it resides in it.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 84.0, "DataCoverage": 86.33333333333333, "Fluency": 84.66666666666667, "Relevance": 82.66666666666667, "TextStructure": 86.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Olga Bondareva, born in Leningrad of the USSR, is an economist and mathematician who died in Saint Petersburg on December 9, 1991.\". The input of model is \"['Olga_Bondareva | birthPlace | Leningrad,_USSR', 'Olga_Bondareva | deathPlace | Saint_Petersburg', 'Olga_Bondareva | deathDate | 1991-12-09', 'Olga_Bondareva | professionalField | Economics', 'Olga_Bondareva | professionalField | Mathematics']\". The model generated output is \"Leningrad, USSR is the birthplace of bakso which is a professional field of Economics. It retired on 1991-12-09 and died in Saint Petersburg.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 79.66666666666667, "DataCoverage": 90.66666666666667, "Fluency": 90.33333333333333, "Relevance": 95.66666666666667, "TextStructure": 90.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Acharya Institute of Technology is in Bangalore, Karnataka.\". The input of model is \"['Acharya_Institute_of_Technology | city | Bangalore', 'Acharya_Institute_of_Technology | state | Karnataka']\". The model generated output is \"The Acharya Institute of Technology is located in Bangalore, Karnataka.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.33333333333333, "DataCoverage": 98.0, "Fluency": 98.33333333333333, "Relevance": 98.66666666666667, "TextStructure": 98.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The 11th Mississippi Infantry Monument is in the United States in a place called the municipality of Gettysburg in Adams County, Pennsylvania. This place has to its southeast Carroll County, Maryland and to its north Pennsylvania's Cumberland County. The 11th Mississippi Infantry Monument is classified as a contributing property.\". The input of model is \"['11th_Mississippi_Infantry_Monument | location | Adams_County,_Pennsylvania', '11th_Mississippi_Infantry_Monument | municipality | Gettysburg,_Pennsylvania', 'Adams_County,_Pennsylvania | hasToItsSoutheast | Carroll_County,_Maryland', '11th_Mississippi_Infantry_Monument | category | Contributing_property', 'Adams_County,_Pennsylvania | hasToItsNorth | Cumberland_County,_Pennsylvania', '11th_Mississippi_Infantry_Monument | country | \"United States\"']\". The model generated output is \"The 11th Mississippi Infantry Monument is located in Gettysburg, Adams County, Pennsylvania, United States. It is categorised as a contributing property and is categorised as a contributing property. To the north of Adams County lies Cumberland County and to the southeast lies Carroll County, Maryland.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 86.0, "DataCoverage": 87.0, "Fluency": 72.66666666666667, "Relevance": 93.66666666666667, "TextStructure": 76.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nurhan Atasoy was born in Re\u015fadiye.\". The input of model is \"Nurhan_Atasoy | birthPlace | Re\u015fadiye\". The model generated output is \"The birth place of NurB\u00fct\u00fcner Atasoy is Re\u015fadi000000.0.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 32.333333333333336, "DataCoverage": 59.0, "Fluency": 56.0, "Relevance": 58.666666666666664, "TextStructure": 70.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The album Turn Me On (which has run time of 35.1 minutes) was preceded by the album Let It Breed; Let It Breed is preceded by the release of the album Love American Style.\". The input of model is \"['Turn_Me_On_(album) | runtime | 35.1', 'Turn_Me_On_(album) | precededBy | Let_It_Breed', 'Let_It_Breed | precededBy | Love_American_Style_(album)']\". The model generated output is \"Turn Me On was preceded by Let On which is the sequel to Let It Breed. It is 35.1 metres large.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 39.333333333333336, "DataCoverage": 65.0, "Fluency": 62.666666666666664, "Relevance": 42.666666666666664, "TextStructure": 61.333333333333336}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Adams County, Pennsylvania, United States is home to the 11th Mississippi Infantry Monument.\". The input of model is \"['11th_Mississippi_Infantry_Monument | location | Adams_County,_Pennsylvania', '11th_Mississippi_Infantry_Monument | country | \"United States\"']\". The model generated output is \"The 11th Mississippi Infantry Monument is located in Adams County, Pennsylvania, United States.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.33333333333333, "DataCoverage": 99.33333333333333, "Fluency": 98.66666666666667, "Relevance": 99.33333333333333, "TextStructure": 98.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Est\u00e1dio Municipal Coaracy da Mata Fonseca is a sports ground which is based in Arapiraca and used by the team Agremia\u00e7\u00e3o Sportiva Arapiraquense. Agremia\u00e7\u00e3o Sportiva Arapiraquense played in the 2015 season of the Brazil based Campeonato Brasileiro S\u00e9rie C league. Vila Nova Futebol Clube have been previous champions of the Campeonato Brasileiro S\u00e9rie C league.\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | season | 2015', 'Campeonato_Brasileiro_S\u00e9rie_C | champions | Vila_Nova_Futebol_Clube', 'Campeonato_Brasileiro_S\u00e9rie_C | country | Brazil', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca']\". The model generated output is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense play in the Campeonato Brasileiro S\u00e9rie C league in 2015. Their ground is the Estadio Municipal Coaracy da Mata Fonseca in Arapiraca, Brazil. Vila Nova Futebol Clube have been champions of Campeonato Brasileiro S\u00e9rie C in 2015.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 96.33333333333333, "DataCoverage": 96.33333333333333, "Fluency": 66.0, "Relevance": 100.0, "TextStructure": 69.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Acharya Institute of Technology which is in India, is affiliated with Visvesvaraya Technological University and was given the Technical Campus status by The All India Council for Technical Education which is in Mumbai. The campus address for the Acharya Institute of Technology is: Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore - 560090.\". The input of model is \"['Acharya_Institute_of_Technology | campus | \"In Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090.\"', 'All_India_Council_for_Technical_Education | location | Mumbai', 'Acharya_Institute_of_Technology | affiliation | Visvesvaraya_Technological_University', 'Acharya_Institute_of_Technology | country | \"India\"', 'Acharya_Institute_of_Technology | wasGivenTheTechnicalCampusStatusBy | All_India_Council_for_Technical_Education']\". The model generated output is \"The Acharya Institute of Technology's campus is in Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090, India. It was given the technical campus status by the All India Council for Technical Education in Mumbai. The Institute is affiliated with the Visvesvaraya Technological University.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.33333333333333, "DataCoverage": 99.33333333333333, "Fluency": 99.0, "Relevance": 99.66666666666667, "TextStructure": 99.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Bootleg Series Volume I: The Quine Tapes was recorded in the United States.\". The input of model is \"Bootleg_Series_Volume_1:_The_Quine_Tapes | recordedIn | United_States\". The model generated output is \"The United States is the recording labels of the asteroid bhajji are on the 1st of July 2016.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 39.666666666666664, "DataCoverage": 44.333333333333336, "Fluency": 41.666666666666664, "Relevance": 42.333333333333336, "TextStructure": 27.333333333333332}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nurhan Atasoy is a resident of Istanbul, Turkey. Istanbul is in the Eastern European Summer Time zone.\". The input of model is \"['Nurhan_Atasoy | residence | Istanbul', 'Istanbul | timeZone | Eastern_European_Time', 'Istanbul | timeZone | Eastern_European_Summer_Time', 'Nurhan_Atasoy | residence | Turkey']\". The model generated output is \"The United States resides in Istanbul which is part of the Eastern European Summer Time zone and lives in Turkey. The country is the location of NurB\u00fct\u00fcner Atasoy.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 47.0, "DataCoverage": 57.666666666666664, "Fluency": 57.0, "Relevance": 85.33333333333333, "TextStructure": 74.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"\"Agremia\u00e7\u00e3o Sportiva Arapiraquense\" is the full name of the Agremia\u00e7\u00e3o Sportiva Arapiraquense club, which has 17000 members. They play in the Campeonato Brasileiro S\u00e9rie C league on their ground Estadio Municipal Coaracy da Mata Fonseca located in Arapiraca. The championship is based in Brazil and Vila Nova Futebol Clube are the champions.\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Campeonato_Brasileiro_S\u00e9rie_C | champions | Vila_Nova_Futebol_Clube', 'Campeonato_Brasileiro_S\u00e9rie_C | country | Brazil', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | numberOfMembers | 17000', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | fullName | \"Agremia\u00e7\u00e3o Sportiva Arapiraquense\"']\". The model generated output is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense's full name is Agremia\u00e7\u00e3o Sportiva Arapiraquense. They play in the Campeonato Brasileiro S\u00e9rie C league which has 17000 members and Vila Nova Futebol Clube as former champions. Their full name is Agremia\u00e7\u00e3o Sportiva Arapiraquense.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 92.66666666666667, "DataCoverage": 89.0, "Fluency": 88.0, "Relevance": 89.66666666666667, "TextStructure": 93.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The full name of the country Israel is the State of Israel.\". The input of model is \"Israel | longName | \"State of Israel\"\". The model generated output is \"Israel's full name is the State of Israel.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 82.66666666666667, "DataCoverage": 66.66666666666667, "Fluency": 95.66666666666667, "Relevance": 67.66666666666667, "TextStructure": 95.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"\"English Without Tears\" was produced and written by Anatole de Grunwald. The editor for the film was Alan Jaggs.\". The input of model is \"['English_Without_Tears | editing | Alan_Jaggs', 'English_Without_Tears | producer | Anatole_de_Grunwald', 'English_Without_Tears | writer | Anatole_de_Grunwald']\". The model generated output is \"Anatole de Grunwald and Alan Junwald are the creators of English (community: English), which was designed by Alan Junwald.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 50.0, "DataCoverage": 42.5, "Fluency": 65.0, "Relevance": 30.0, "TextStructure": 75.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Dr. G. P. Prabhukumar is the director of the Acharya Institute of Technology. Its address is Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore - 560090. The All India Council for Technical Education, located in Mumbai, was proud to give the Technical Campus status to the Acharya Institute of Technology.\". The input of model is \"['Acharya_Institute_of_Technology | campus | \"In Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090.\"', 'All_India_Council_for_Technical_Education | location | Mumbai', 'Acharya_Institute_of_Technology | director | \"Dr. G. P. Prabhukumar\"', 'Acharya_Institute_of_Technology | city | Bangalore', 'Acharya_Institute_of_Technology | wasGivenTheTechnicalCampusStatusBy | All_India_Council_for_Technical_Education']\". The model generated output is \"The Acharya Institute of Technology's campus is in Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090. It was given the technical campus by the All India Council for Technical Education in Mumbai. The director of the Institute is Dr. G. P. Prabhukumar.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 93.66666666666667, "DataCoverage": 95.66666666666667, "Fluency": 83.33333333333333, "Relevance": 99.33333333333333, "TextStructure": 90.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Hypermarcas, a type of S.A. (corporation), was founded on January 1, 2001, and has a subsidiary called Mantecorp.\". The input of model is \"['Hypermarcas | foundingDate | 2001-01-01', 'Hypermarcas | type | S.A._(corporation)', 'Hypermarcas | subsidiary | Mantecorp']\". The model generated output is \"Hypermarcas, a type of S.A. corporation founded on January 1, 2001, has a subsidiary called Mantecorp.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 81.5, "DataCoverage": 86.0, "Fluency": 70.5, "Relevance": 78.0, "TextStructure": 80.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The number of employees at Hypermarcas is 10252.\". The input of model is \"Hypermarcas | numberOfEmployees | 10252\". The model generated output is \"Hypermarcas has 10252 employees.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.0, "DataCoverage": 97.33333333333333, "Fluency": 99.0, "Relevance": 97.33333333333333, "TextStructure": 98.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nurhan Atasoy, born January 1st, 1934, was of Turkish nationality.\". The input of model is \"['Nurhan_Atasoy | birthDate | 1934-01-01', 'Nurhan_Atasoy | nationality | Turkish_people']\". The model generated output is \"NurB\u00fct\u00fcner Atasoy was born on 1934-01-01 and is a national of the Turkish people.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 97.0, "DataCoverage": 98.0, "Fluency": 87.33333333333333, "Relevance": 97.33333333333333, "TextStructure": 92.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Pontiac Rageous went into production in 1997 and ended the same year.\". The input of model is \"['Pontiac_Rageous | productionStartYear | 1997', 'Pontiac_Rageous | productionEndYear | 1997']\". The model generated output is \"Pontiac Rageous, production of the Pontiac Rageous began in 1997.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 53.0, "DataCoverage": 49.5, "Fluency": 48.5, "Relevance": 73.5, "TextStructure": 61.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Fellowship of the Ring belongs to the fantasy genre.\". The input of model is \"The_Fellowship_of_the_Ring | literaryGenre | Fantasy_(genre)\". The model generated output is \"The Fantasy genre is the genre of the Fellowship of the Ring.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 62.0, "DataCoverage": 69.0, "Fluency": 52.666666666666664, "Relevance": 72.66666666666667, "TextStructure": 62.333333333333336}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The College of William and Mary is the owner of the Alan B. Miller Hall which was completed on 1st June 2009, address is \"101 Ukrop Way\". The Mason School of Business are the current tenants of Alan B Miller Hall.\". The input of model is \"['Alan_B._Miller_Hall | address | \"101 Ukrop Way\"', 'Alan_B._Miller_Hall | currentTenants | Mason_School_of_Business', 'Alan_B._Miller_Hall | completionDate | 2009-06-01', 'Alan_B._Miller_Hall | owner | College_of_William_&_Mary']\". The model generated output is \"The Mason School of Business is the current tenants of Alan B. Miller Hall which is owned by The College of William and Mary and completed on 1 June 2009 at 101 Ukrop Way.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 97.33333333333333, "DataCoverage": 96.0, "Fluency": 92.33333333333333, "Relevance": 94.66666666666667, "TextStructure": 93.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The American band, the Honeymoon Killers, self-produced its album Turn Me On, with the help of some musicians from an associated musical group called Pussy Galore.\". The input of model is \"['The_Honeymoon_Killers_(American_band) | associatedBand/associatedMusicalArtist | Pussy_Galore_(band)', 'Turn_Me_On_(album) | producer | The_Honeymoon_Killers_(American_band)']\". The model generated output is \"Turn Me On was manufactured by The Honeoon Killers, an American band and associated with the musical artist Pussy Galore.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 74.66666666666667, "DataCoverage": 81.66666666666667, "Fluency": 80.0, "Relevance": 82.0, "TextStructure": 85.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Adam West was born in the year 1928.\". The input of model is \"Adam_West | birthYear | 1928\". The model generated output is \"Adam West was born in 1928.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 100.0, "Relevance": 100.0, "TextStructure": 100.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Bionico dessert is a food found in Mexico (currency: peso). Sandesh is also a dessert.\". The input of model is \"['Mexico | currency | Mexican_peso', 'Bionico | course | Dessert', 'Dessert | dishVariation | Sandesh_(confectionery)', 'Bionico | country | Mexico']\". The model generated output is \"Sandesh (confectionery) and Bionico are types of dessert. The latter dish comes from Mexico where the currency is the peso.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 86.66666666666667, "DataCoverage": 93.66666666666667, "Fluency": 80.33333333333333, "Relevance": 87.33333333333333, "TextStructure": 81.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy has 16,800 undergraduate students.\". The input of model is \"University_of_Burgundy | numberOfUndergraduateStudents | 16800\". The model generated output is \"There are 16800 undergraduate students at the University of Burgundy.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.66666666666667, "DataCoverage": 98.33333333333333, "Fluency": 97.66666666666667, "Relevance": 97.33333333333333, "TextStructure": 98.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Death on a Factory Farm, produced and directed by Tom Simon and edited by Geof Bartz was broadcast by HBO. The music was composed by Jamie Lawrence and Sarah Teale was co-producer.\". The input of model is \"['Death_on_a_Factory_Farm | director | Tom_Simon', 'Death_on_a_Factory_Farm | producer | Tom_Simon', 'Death_on_a_Factory_Farm | broadcastedBy | HBO', 'Death_on_a_Factory_Farm | editor | Geof_Bartz', 'Death_on_a_Factory_Farm | musicComposer | Jamie_Lawrence', 'Death_on_a_Factory_Farm | producer | Sarah_Teale']\". The model generated output is \"Tom Simon and Jamie Lawrence are the creators of Death on airbase. It was created by Tom Simon and includes composer Jamie Lawrence.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 44.666666666666664, "DataCoverage": 51.666666666666664, "Fluency": 42.333333333333336, "Relevance": 51.333333333333336, "TextStructure": 44.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nurhan Atasoy, a Turkish national residing in Istanbul was born on 1934-01-01 in Turkey.\". The input of model is \"['Nurhan_Atasoy | birthDate | 1934-01-01', 'Nurhan_Atasoy | residence | Istanbul', 'Nurhan_Atasoy | citizenship | Turkey', 'Nurhan_Atasoy | birthPlace | Turkey']\". The model generated output is \"Nur-speed Atasoy was born in Turkey on 1934-01-01 and lives in Istanbul. It is a citizenship found in Turkey.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 82.0, "DataCoverage": 99.33333333333333, "Fluency": 62.333333333333336, "Relevance": 87.0, "TextStructure": 69.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The band Train followed up \"This'll be my year\" with a 3 minutes, 16 seconds long pop rock song, entitled \"Mermaid.\" Mermaid, which is also considered an example of the reggae genre of music, was written by songwriter, Espen Lind and released on December, 27, 2012.\". The input of model is \"['Mermaid_(Train_song) | genre | Pop_rock', 'Mermaid_(Train_song) | releaseDate | 2012-12-27', \"Mermaid_(Train_song) | precededBy | This'll_Be_My_Year\", 'Mermaid_(Train_song) | writer | Espen_Lind', 'Mermaid_(Train_song) | runtime | 3.16', 'Mermaid_(Train_song) | genre | Reggae']\". The model generated output is \"The musical genre of bakewell (19255) is pop rock. It was created by Espen Lind and its release date is on 27 December 2012. It was preceded by The'll AarhMy and its genre is Reggae.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 82.66666666666667, "DataCoverage": 77.0, "Fluency": 77.66666666666667, "Relevance": 82.0, "TextStructure": 71.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The periapsis of 1147 Stavropolis is 260855000000.\". The input of model is \"1147_Stavropolis | periapsis | 260855000000.0\". The model generated output is \"The periapsis of bakewell is 26085000000.0.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 54.0, "DataCoverage": 87.33333333333333, "Fluency": 86.0, "Relevance": 60.666666666666664, "TextStructure": 83.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Est\u00e1dio Municipal Coaracy da Mata Fonseca is located in Arapiraca. It is the ground of Agremia\u00e7\u00e3o Sportiva Arapiraquense who play in the Campeonato Brasileiro S\u00e9rie C league and have 17000 members. The full name of the club is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense\"\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | numberOfMembers | 17000', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | fullName | \"Agremia\u00e7\u00e3o Sportiva Arapiraquense\"']\". The model generated output is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense's full name is Agremia\u00e7\u00e3o Sportiva Arapiraquense. They play in the Campeonato Brasileiro S\u00e9rie C league and have 17000 members. Their ground is the Estadio Municipal Coaracy da Mata Fonseca which is located in Arapiraca.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 85.0, "DataCoverage": 81.0, "Fluency": 60.0, "Relevance": 82.0, "TextStructure": 69.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Aaron Turner is a post-metal musician from Massachusetts who plays the electric guitar. He started performing in 1995 and is a musician in the Lotus Eaters band.\". The input of model is \"['Aaron_Turner | genre | Post-metal', 'Aaron_Turner | associatedBand/associatedMusicalArtist | Lotus_Eaters_(band)', 'Aaron_Turner | origin | Massachusetts', 'Aaron_Turner | instrument | Electric_guitar', 'Aaron_Turner | activeYearsStartYear | 1995']\". The model generated output is \"Aaron Turner is a performer of the musical genre Post metal and he is a musician in the Lotus Eaters band. He plays electric guitar and started performing in 1995.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.33333333333333, "DataCoverage": 84.33333333333333, "Fluency": 84.66666666666667, "Relevance": 93.0, "TextStructure": 99.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The music of the film English Without Tears is composed by Nicholas Brodszky with Harold French as the director.\". The input of model is \"['English_Without_Tears | musicComposer | Nicholas_Brodszky', 'English_Without_Tears | director | Harold_French']\". The model generated output is \"Nicholas Brodszky is the composer of English Gaussia, whose director is Harold French.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 78.66666666666667, "DataCoverage": 77.0, "Fluency": 64.33333333333333, "Relevance": 95.0, "TextStructure": 82.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Andrzej Piotr Ruszczy\u0144ski is known for his work with stochastic programming.\". The input of model is \"Andrzej_Piotr_Ruszczy\u0144ski | knownFor | Stochastic_programming\". The model generated output is \"Andrze\u015fotr Ruszczy-speed ski is known for Stochastic programming.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 62.333333333333336, "DataCoverage": 86.33333333333333, "Fluency": 74.0, "Relevance": 73.33333333333333, "TextStructure": 85.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Abraham Ribicoff was born in the USA. He was married to Casey Ribicoff and was succeeded by John N. Dempsey.\". The input of model is \"['Abraham_A._Ribicoff | spouse | Casey_Ribicoff', 'Abraham_A._Ribicoff | successor | John_N._Dempsey', 'Abraham_A._Ribicoff | birthPlace | United_States']\". The model generated output is \"Abraham A. Ribicoff was born in the United States and was married to Casey Ribicoff. His successor was John N. Dempsey.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 100.0, "Relevance": 100.0, "TextStructure": 100.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Aleksandr Prudnikov plays for FC Terek Grozny (based at Grozny) and managed by Rashid Rakhimov.Aleksandr Prudnikov played for FC Spartak Moscow's youth team an his current club is FC Amkar Perm.\". The input of model is \"['FC_Terek_Grozny | ground | Grozny', 'Aleksandr_Prudnikov | currentclub | FC_Amkar_Perm', 'Aleksandr_Prudnikov | club | FC_Terek_Grozny', 'FC_Terek_Grozny | manager | Rashid_Rakhimov', 'Aleksandr_Prudnikov | youthclub | FC_Spartak_Moscow']\". The model generated output is \"Aleksandr Prudnikov is currently playing for FC Amkar Perm and FC Terek Grozny. Rashid Rakhimov is the manager of FC Terek Grozny, the ground of which, is located in Grozny.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 61.333333333333336, "DataCoverage": 66.66666666666667, "Fluency": 73.0, "Relevance": 77.0, "TextStructure": 68.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Fellowship of the Ring was written by J.R.R Tolkien. The Two Towers is the sequel to it. It was preceded by the Hobbit. The Hobbit's genre is juvenile fantasy and literary genre is high fantasy.\". The input of model is \"['The_Fellowship_of_the_Ring | followedBy | The_Two_Towers', 'The_Hobbit | literaryGenre | Juvenile_fantasy', 'The_Hobbit | literaryGenre | High_fantasy', 'The_Fellowship_of_the_Ring | author | J._R._R._Tolkien', 'The_Fellowship_of_the_Ring | precededBy | The_Hobbit']\". The model generated output is \"The Fellowship of the Ring was preceded by The Hobbit which is considered a High fantasy. It was written by JR Tolkien and followed by The Two Towers.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 83.0, "Fluency": 92.33333333333333, "Relevance": 96.0, "TextStructure": 86.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Brandon Carter born in England attended University of Cambridge whose vice chancellor is Leszek Borysiewicz.\". The input of model is \"['Brandon_Carter | birthPlace | England', 'Brandon_Carter | almaMater | University_of_Cambridge', 'University_of_Cambridge | viceChancellor | Leszek_Borysiewicz']\". The model generated output is \"Brandon Carter was born in England and graduated from the University of Cambridge where Leszek Borysiewicz was vice chancellor.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 96.66666666666667, "Fluency": 100.0, "Relevance": 96.66666666666667, "TextStructure": 96.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Velvet Underground Squeeze album was succeeded by the rock album Bootleg Series Volume 1: The Quine Tapes, recorded under record label Polydor Records in San Francisco.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | genre | Rock_music', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | precededBy | Squeeze_(The_Velvet_Underground_album)', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Polydor_Records', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | recordedIn | San_Francisco']\". The model generated output is \"Located in San Francisco, and signed to the record label Polydor Records, bhajji: The Velvet Underground is preceded by Squeeze (The Velvet Underground) and The record label is Polydor Records. Their genre is rock music.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 71.66666666666667, "DataCoverage": 90.66666666666667, "Fluency": 68.66666666666667, "Relevance": 90.33333333333333, "TextStructure": 72.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Abdul Taib Mahmud, member of the Parti Pesaka Bumiputera Bersatu party, resides in Demak Jaya, Jalan Bako, Kuching, Sarawak.\". The input of model is \"['Abdul_Taib_Mahmud | residence | \"Demak Jaya, Jalan Bako, Kuching, Sarawak\"', 'Abdul_Taib_Mahmud | party | Parti_Pesaka_Bumiputera_Bersatu']\". The model generated output is \"Abdul Taib Mahmud lives in Demak Jaya, Jalan Bako, Kuching, Sarawak and is a member of the Parti Pesaka Bumiputera Bersatu.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.66666666666667, "DataCoverage": 100.0, "Fluency": 79.66666666666667, "Relevance": 99.0, "TextStructure": 85.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Reggae's stylistic origin is rhythm and blues.\". The input of model is \"Reggae | stylisticOrigin | Rhythm_and_Blues\". The model generated output is \"Reggae originated from rhythm and blues music.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 91.66666666666667, "DataCoverage": 100.0, "Fluency": 94.33333333333333, "Relevance": 100.0, "TextStructure": 90.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Pontiac Rageous was a car that went into production in 1997 and was manufactured by Pontiac. Pontiac was founded in the United States, and one of the successors of Pontiac is Buick.\". The input of model is \"['Pontiac_Rageous | manufacturer | Pontiac', 'Pontiac | foundationPlace | United_States', 'Pontiac_Rageous | productionStartYear | 1997', 'Pontiac | successor | Buick']\". The model generated output is \"Pontiac, founded in the United States, manufactured the Pontiac Rageous production start in 1997. The successor of Pontiac is Buick.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 74.33333333333333, "DataCoverage": 87.66666666666667, "Fluency": 59.0, "Relevance": 84.66666666666667, "TextStructure": 56.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The 89-minute-long movie English Without Tears includes music composed by Nicholas Brodszky. Alan Jaggs was in charge of the editing.\". The input of model is \"['English_Without_Tears | musicComposer | Nicholas_Brodszky', 'English_Without_Tears | editing | Alan_Jaggs', 'English_Without_Tears | runtime | 89.0']\". The model generated output is \"Nicholas Brodszky is the composer of English bakewell and it was built by Alan Jaggs. It has an apoapsis of 89.0.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 26.0, "DataCoverage": 33.0, "Fluency": 44.666666666666664, "Relevance": 28.0, "TextStructure": 50.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Est\u00e1dio Municipal Coaracy da Mata Fonseca is a stadium in Arapiraca. It is the home ground of Agremia\u00e7\u00e3o Sportiva Arapiraquense of the Brazilian Championship Series C where the champions are Vila Nova Futebol Clube. The Brazilian name for the championship is Campeonato Brasileiro S\u00e9rie C, and Agremia\u00e7\u00e3o Sportiva Arapiraquense competed there in 2015. The club has 17,000 members.\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | season | 2015', 'Campeonato_Brasileiro_S\u00e9rie_C | champions | Vila_Nova_Futebol_Clube', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | numberOfMembers | 17000', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | fullName | \"Agremia\u00e7\u00e3o Sportiva Arapiraquense\"']\". The model generated output is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense's full name is Agremia\u00e7\u00e3o Sportiva Arapiraquense. They play in the Campeonato Brasileiro S\u00e9rie C league in 2015 and have 17000 members. Their ground is the Estadio Municipal Coaracy da Mata Fonseca which is located in Arapiraca. Vila Nova Futebol Clube have been champions of Campeonato Brasileiro Serie C in 2015.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.0, "DataCoverage": 99.5, "Fluency": 92.0, "Relevance": 99.0, "TextStructure": 94.5}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Darinka Dentcheva is resident of new Jersey which has an area of water that is 3.544.04 meters squared and citizen of the United States where the motto is \"In God We Trust\".\". The input of model is \"['Darinka_Dentcheva | residence | New_Jersey', 'New_Jersey | areaOfWater | 3544040000.0', 'Darinka_Dentcheva | residence | United_States', 'Darinka_Dentcheva | citizenship | United_States', 'United_States | motto | \"In God we trust\"']\". The model generated output is \"God we trust is the motto of the United States where they reside in New Jersey. The country has a water area of 35040000.0 and reside in the United States. The country is the location of Alpharetta inka Dentcheva, a citizenship.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 81.0, "DataCoverage": 85.0, "Fluency": 87.0, "Relevance": 80.33333333333333, "TextStructure": 73.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Bootleg Series Volume I: The Quine Tapes's genre is rock, of which proto-punk is a musical subgenre.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | genre | Rock_music', 'Rock_music | musicSubgenre | Proto-punk']\". The model generated output is \"Proto-punk is a subgenre of rock music which is the genre of bhajji pedas 1.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 60.333333333333336, "DataCoverage": 81.66666666666667, "Fluency": 71.33333333333333, "Relevance": 76.33333333333333, "TextStructure": 76.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The architect of Alan B Miller Hall, owned by The College of William and Mary, is Robert A M Stern and it is situated at 101 Ukrop Way, Virginia, USA. They started to build the Alan B Miller Hall on the 30th of March, 2007 and The Mason School of Business are now its tenants.\". The input of model is \"['Alan_B._Miller_Hall | architect | Robert_A._M._Stern', 'Alan_B._Miller_Hall | address | \"101 Ukrop Way\"', 'Alan_B._Miller_Hall | buildingStartDate | \"30 March 2007\"', 'Alan_B._Miller_Hall | currentTenants | Mason_School_of_Business', 'Alan_B._Miller_Hall | location | Virginia', 'Mason_School_of_Business | country | United_States', 'Alan_B._Miller_Hall | owner | College_of_William_&_Mary']\". The model generated output is \"The Mason School of Business, located in Virginia, United States, are the current tenants of Alan B. Miller Hall at 101 Ukrop Way. Alan B. Miller Hall was designed by Robert A M Stern and retired on 30 March 2007. The Hall is owned by the College of William and Mary.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 79.66666666666667, "DataCoverage": 73.33333333333333, "Fluency": 91.33333333333333, "Relevance": 77.0, "TextStructure": 94.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy is located in Dijon in France, which is also known as the French Republic and led by Claude Bartolone.\". The input of model is \"['University_of_Burgundy | country | France', 'France | longName | \"French Republic\"', 'University_of_Burgundy | city | Dijon', 'France | leader | Claude_Bartolone']\". The model generated output is \"Claude Bartolone is the leader of France where the French Republic is also known as the University of Burgundy. The university is located in Dijon.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 87.33333333333333, "DataCoverage": 93.33333333333333, "Fluency": 82.0, "Relevance": 92.66666666666667, "TextStructure": 91.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Though associated with Jwaydan Moyine, trance music was the genre of choice for Andrew Rayel.\". The input of model is \"['Andrew_Rayel | genre | Trance_music', 'Andrew_Rayel | associatedBand/associatedMusicalArtist | Jwaydan_Moyine']\". The model generated output is \"Andrew Rayel plays Trance music and is associated with the musical artist Jwaydan Moyine.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.33333333333333, "DataCoverage": 96.33333333333333, "Fluency": 92.33333333333333, "Relevance": 96.33333333333333, "TextStructure": 99.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy has a 2900 member staff. The university, located in Dijon, serves 1299 doctoral students.\". The input of model is \"['University_of_Burgundy | staff | 2900', 'University_of_Burgundy | numberOfDoctoralStudents | 1299', 'University_of_Burgundy | campus | Dijon', 'University_of_Burgundy | city | Dijon']\". The model generated output is \"The University of Burgundy in Dijon has 2900 staff and 1299 doctoral students. Its campus is located in the city of Dijon.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.33333333333333, "DataCoverage": 95.0, "Fluency": 91.33333333333333, "Relevance": 95.66666666666667, "TextStructure": 89.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The rock album \"Bootleg Series Volume 1: The Quine Tapes\" was produced by proto-punk band The Velvet Underground.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | producer | The_Velvet_Underground', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | genre | Rock_music', 'The_Velvet_Underground | genre | Proto-punk']\". The model generated output is \"The Velvet Underground, made by The Velvet Underground, is made from rock music and is the genre of Proto-punk.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 58.0, "DataCoverage": 50.5, "Fluency": 68.5, "Relevance": 57.0, "TextStructure": 75.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Death on a Factory Farm is a film broadcasted by HBO, formerly known as The Green Channel.\". The input of model is \"['Death_on_a_Factory_Farm | broadcastedBy | HBO', 'HBO | formerName | The Green Channel']\". The model generated output is \"HBO (formerly known as The Green Channel) is broadcast by HBO.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 94.33333333333333, "DataCoverage": 95.0, "Fluency": 95.66666666666667, "Relevance": 95.66666666666667, "TextStructure": 93.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Lancashire native Bill Oddie starred in Bananaman, a story created by Steve Bright filmed in London at Broadcasting House, the home of the BBC who also broadcast it.\". The input of model is \"['BBC | city | Broadcasting_House', 'Bananaman | starring | Bill_Oddie', 'Bananaman | creator | Steve_Bright', 'Bananaman | broadcastedBy | BBC', 'Bill_Oddie | birthPlace | Lancashire']\". The model generated output is \"Bananaman was created by Steve Bright and starred Bill Oddie who was born in Lancashire. It was created by the BBC which is based in the city of Broadcasting House.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 83.33333333333333, "DataCoverage": 85.66666666666667, "Fluency": 92.0, "Relevance": 92.33333333333333, "TextStructure": 81.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Ciudad Ayala (a city in Mexico) has a leader called the Governator and it is part of Morelos.\". The input of model is \"['Ciudad_Ayala | leaderTitle | Governator', 'Ciudad_Ayala | isPartOf | Morelos', 'Ciudad_Ayala | country | Mexico']\". The model generated output is \"The leader of bakewell (part of Morelos) is called a Governator and it is found in Mexico.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 55.0, "DataCoverage": 50.0, "Fluency": 88.5, "Relevance": 52.0, "TextStructure": 88.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"One of the leaders of Israel is Reuven Rivlin.\". The input of model is \"Israel | leader | Reuven_Rivlin\". The model generated output is \"The leader of Israel is Reuven Rivlin.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.0, "DataCoverage": 99.33333333333333, "Fluency": 99.66666666666667, "Relevance": 99.33333333333333, "TextStructure": 99.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The College of William and Mary, located in Williamsburg Virginia is the location of the Mason School of Business located inside Alan B Miller Hall.\". The input of model is \"['Alan_B._Miller_Hall | currentTenants | Mason_School_of_Business', 'Alan_B._Miller_Hall | location | Williamsburg,_Virginia', 'Alan_B._Miller_Hall | owner | College_of_William_&_Mary']\". The model generated output is \"The Mason School of Business are the current tenants of Alan B. Miller Hall, Williamsburg, Virginia, which is owned by The College of William and Mary.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 99.5, "Fluency": 82.5, "Relevance": 100.0, "TextStructure": 85.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"GMA New Media, Inc. is a subsidiary of GMA Network.\". The input of model is \"GMA_New_Media | parentCompany | GMA_Network_(company)\". The model generated output is \"GMA New Media is a subsidiary of GMA Network.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.0, "DataCoverage": 70.66666666666667, "Fluency": 97.66666666666667, "Relevance": 70.33333333333333, "TextStructure": 97.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy in Dijon has 2,900 staff members along with 27,400 students, of which 9,400 are postgraduate students and 1,299 are PhD students.\". The input of model is \"['University_of_Burgundy | staff | 2900', 'University_of_Burgundy | numberOfDoctoralStudents | 1299', 'University_of_Burgundy | campus | Dijon', 'University_of_Burgundy | numberOfStudents | 27400', 'University_of_Burgundy | numberOfPostgraduateStudents | 9400']\". The model generated output is \"The University of Burgundy has 2900 students and 9400 postgraduate students. Its campus is in Dijon and it has 27400 students.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.0, "DataCoverage": 91.66666666666667, "Fluency": 88.66666666666667, "Relevance": 83.0, "TextStructure": 86.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense's full name is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense\" and they played in the Campeonato Brasileiro S\u00e9rie C league in the 2015 season from Brazil and Vila Nova Futebol Clube have been champions. The Est\u00e1dio Municipal Coaracy da Mata Fonseca is the name of the ground of Agremia\u00e7\u00e3o Sportiva Arapiraquense located in Arapiraca.\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | season | 2015', 'Campeonato_Brasileiro_S\u00e9rie_C | champions | Vila_Nova_Futebol_Clube', 'Campeonato_Brasileiro_S\u00e9rie_C | country | Brazil', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | fullName | \"Agremia\u00e7\u00e3o Sportiva Arapiraquense\"']\". The model generated output is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense's full name is Agremia\u00e7\u00e3o Sportiva Arapiraquense. They play in the Campeonato Brasileiro S\u00e9rie C league in 2015. Their ground is the Estadio Municipal Coaracy da Mata Fonseca which is located in Arapiraca, Brazil. Vila Nova Futebol Clube have been champions of Campeonato Brasileiro S\u00e9rie C.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 84.33333333333333, "DataCoverage": 88.33333333333333, "Fluency": 73.33333333333333, "Relevance": 89.66666666666667, "TextStructure": 76.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy has 9400 post-graduate students and employs 2900 staff members.\". The input of model is \"['University_of_Burgundy | staff | 2900', 'University_of_Burgundy | numberOfPostgraduateStudents | 9400']\". The model generated output is \"The University of Burgundy has 2900 staff and 9400 postgraduate students.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 100.0, "Relevance": 100.0, "TextStructure": 100.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Lady Anne Monson was born in the Kingdom of England whose capital was previously Winchester.\". The input of model is \"['Lady_Anne_Monson | birthPlace | Kingdom_of_England', 'Kingdom_of_England | capital | Winchester']\". The model generated output is \"The capital of England is Win000000 and the country is the birthplace of bhajji pedas.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 24.5, "DataCoverage": 36.5, "Fluency": 15.0, "Relevance": 27.5, "TextStructure": 21.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy is located in the city of Dijon in France.\". The input of model is \"['University_of_Burgundy | city | Dijon', 'University_of_Burgundy | country | France']\". The model generated output is \"The University of Burgundy is located in the city of Dijon, France.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 91.66666666666667, "DataCoverage": 93.33333333333333, "Fluency": 86.0, "Relevance": 94.0, "TextStructure": 90.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Ciudad Ayala is in Morelos and has a population density of 1604.\". The input of model is \"['Ciudad_Ayala | populationDensity | 1604.0', 'Ciudad_Ayala | isPartOf | Morelos']\". The model generated output is \"The population density of bakewell (part of Morelos) is 1604.0.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 70.66666666666667, "DataCoverage": 82.33333333333333, "Fluency": 81.0, "Relevance": 81.33333333333333, "TextStructure": 84.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Mermaid,performed by Train, is on the Sony Music Entertainment label;however,John Lennon also released Imagine which was followed by Happy Xmas (War is Over) on the same record label.\". The input of model is \"['Mermaid_(Train_song) | recordLabel | Sony_Music_Entertainment', 'Mermaid_(Train_song) | followedBy | Imagine_(John_Lennon_song)', 'Imagine_(John_Lennon_song) | followedBy | Happy_Xmas_(War_Is_Over)']\". The model generated output is \"Sony Music Entertainment is the record label of bhajji (Harrietstown) which was followed by Haputale (John Lennon) and Happy Xmas (War Is Over).\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 17.333333333333332, "DataCoverage": 23.666666666666668, "Fluency": 34.0, "Relevance": 10.666666666666666, "TextStructure": 37.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"1147 Stavropolis is an asteroid, discovered on 11 June 1929 by Grigory Neujmin. Its absolute magnitude is 11.5, and it has a rotation period of 20378.5. Its periapsis is 260855000000.0, orbital period is 1249.6 and the apoapsis is 418476000000.0 AU.\". The input of model is \"['1147_Stavropolis | apoapsis | 418476000000.0', '1147_Stavropolis | orbitalPeriod | 1249.6', '1147_Stavropolis | discoverer | Grigory_Neujmin', '1147_Stavropolis | absoluteMagnitude | 11.5', '1147_Stavropolis | discovered | 1929-06-11', '1147_Stavropolis | rotationPeriod | 20378.5', '1147_Stavropolis | periapsis | 260855000000.0']\". The model generated output is \"Grigory Neujmin discovered the celestial body known as arem-arem which has an absolute magnitude of 11.5 and an apoapsis of 418476000000.0. It has an orbital period of 1249.6 and a periapsis of 26085000000.0. It has a rotation period of 20378.5.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 67.0, "DataCoverage": 78.66666666666667, "Fluency": 66.66666666666667, "Relevance": 70.66666666666667, "TextStructure": 85.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Lionsgate provides digital distribution service.\". The input of model is \"Lionsgate | service | Digital_distribution\". The model generated output is \"The digital distribution of bakonsgate is Digital distribution.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 47.666666666666664, "DataCoverage": 35.666666666666664, "Fluency": 35.333333333333336, "Relevance": 48.333333333333336, "TextStructure": 51.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"George Allen & Unwin published The Fellowship of the Ring, a fantasy, on July 29th, 1954. The Fellowship of the Ring is preceded by the Hobbit and followed by The Two Towers.\". The input of model is \"['The_Fellowship_of_the_Ring | publisher | George_Allen_&_Unwin', 'The_Fellowship_of_the_Ring | literaryGenre | Fantasy_(genre)', 'The_Fellowship_of_the_Ring | followedBy | The_Two_Towers', 'The_Fellowship_of_the_Ring | releaseDate | 1954-07-29', 'The_Fellowship_of_the_Ring | precededBy | The_Hobbit']\". The model generated output is \"The Fellowship of the Ring was published by George Allen & Unwin and is followed by The Two Towers. It was preceded by The Hobbit and its musical genre is Fantasy. It retired on July 9, 1954.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 61.0, "DataCoverage": 82.0, "Fluency": 72.66666666666667, "Relevance": 82.33333333333333, "TextStructure": 78.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"A monument to the 11th Mississippi Infantry was erected in 2000 categorised as a contributing property and stands in the state of Pennsylvania in the municipality of Gettysburg in Pennsylvania located in Adams County, Pennsylvania. Carroll County Maryland is southeast and Pennsylvania's Cumberland County is to the north of the Adams County.\". The input of model is \"['11th_Mississippi_Infantry_Monument | established | 2000', '11th_Mississippi_Infantry_Monument | location | Adams_County,_Pennsylvania', '11th_Mississippi_Infantry_Monument | state | \"Pennsylvania\"', '11th_Mississippi_Infantry_Monument | municipality | Gettysburg,_Pennsylvania', 'Adams_County,_Pennsylvania | hasToItsSoutheast | Carroll_County,_Maryland', '11th_Mississippi_Infantry_Monument | category | Contributing_property', 'Adams_County,_Pennsylvania | hasToItsNorth | Cumberland_County,_Pennsylvania']\". The model generated output is \"The 11th Mississippi Infantry Monument is located in Gettysburg, Adams County, Pennsylvania. It was established in 2000 and categorised as a contributing property. Adams County has Carroll County, Maryland to its southeast and Cumberland County, Pennsylvania to its north.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 90.66666666666667, "DataCoverage": 96.0, "Fluency": 69.0, "Relevance": 96.0, "TextStructure": 95.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Train song Mermaid is followed by the John Lennon song Imagine.\". The input of model is \"Mermaid_(Train_song) | followedBy | Imagine_(John_Lennon_song)\". The model generated output is \"The asteroid called bakewell was followed by Imagine John Lennon (V-2).\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 16.666666666666668, "DataCoverage": 67.33333333333333, "Fluency": 67.0, "Relevance": 52.333333333333336, "TextStructure": 66.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Piotr Hallmann is 175.26 cm tall and weighs 70.308 kg.\". The input of model is \"['Piotr_Hallmann | height | 175.26', 'Piotr_Hallmann | weight | 70.308']\". The model generated output is \"The weight of piotr Hall-2is 70.308 and it is 175.26 metres tall.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 55.666666666666664, "DataCoverage": 81.33333333333333, "Fluency": 64.33333333333333, "Relevance": 76.33333333333333, "TextStructure": 83.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Brandon Carter, known for the Carter constant and No-hair theorem, is a professional in the field of general relativity.\". The input of model is \"['Brandon_Carter | professionalField | General_relativity', 'Brandon_Carter | knownFor | Carter_constant', 'Brandon_Carter | knownFor | No-hair_theorem']\". The model generated output is \"Brandon Carter is a carter who professional field is General relativity. He is known for No-hair theorem and is known as a carter.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 87.5, "DataCoverage": 100.0, "Fluency": 82.0, "Relevance": 95.5, "TextStructure": 91.5}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"In the United States, one of the national ethnic groups are the African Americans. The United States is where Abraham A. Ribicoff, who was married to Casey Ribicoff, died.\". The input of model is \"['Abraham_A._Ribicoff | spouse | Casey_Ribicoff', 'Abraham_A._Ribicoff | deathPlace | United_States', 'United_States | ethnicGroup | African_Americans']\". The model generated output is \"Abraham A. Ribicoff was married to Casey Ribicoff and died in the United States where African Americans are an ethnic group.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 86.0, "DataCoverage": 96.5, "Fluency": 74.0, "Relevance": 94.5, "TextStructure": 94.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Super Capers, a movie directed by Ray Griggs, was distributed by both Lionsgate and Roadside Attractions and had a 98 minute run time.\". The input of model is \"['Super_Capers | distributor | Lionsgate', 'Super_Capers | director | Ray_Griggs_(director)', 'Super_Capers | runtime | 98.0', 'Super_Capers | distributor | Roadside_Attractions']\". The model generated output is \"Ray Griggs is Distributed by Ray Griggs. The distributor of the distributor of the distributor of the distributor of the distributor of the distributor of the town of Super Clematis is Roadside Adrakov and it has 98.0 meters as its Distributor.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 58.0, "DataCoverage": 57.666666666666664, "Fluency": 58.666666666666664, "Relevance": 58.666666666666664, "TextStructure": 61.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Hypermarcas, a company that makes healthcare products and sells cosmetics, employs 10,252 people. Hypermarcas had a net income of $108,600,000 and its revenue is $1,800,000,000.\". The input of model is \"['Hypermarcas | netIncome | 108600000', 'Hypermarcas | product | Healthcare', 'Hypermarcas | product | Cosmetics', 'Hypermarcas | numberOfEmployees | 10252', 'Hypermarcas | revenue | 1800000000']\". The model generated output is \"Hypermarcas is a company that makes healthcare products and cosmetics produces healthcare products. They employ 10,252 people and earns $1,800,000,000 a year. Their net income is $108,600,000 and their revenue is $1,800,000,000.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 96.66666666666667, "DataCoverage": 96.66666666666667, "Fluency": 72.66666666666667, "Relevance": 95.66666666666667, "TextStructure": 80.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The city of Bangalore, founded in the state of Karnataka, India By Kempe Gowda I, is home to the Acharya Institute of Technology. Its campus with Technical Status designated by the All India Technical Council for Technical Education in Mumbai, is located at Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090, and the institute is directed by Dr. G.P. Prabhukumar.\". The input of model is \"['Acharya_Institute_of_Technology | campus | \"In Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090.\"', 'All_India_Council_for_Technical_Education | location | Mumbai', 'Acharya_Institute_of_Technology | director | \"Dr. G. P. Prabhukumar\"', 'Bangalore | founder | Kempe_Gowda_I', 'Acharya_Institute_of_Technology | city | Bangalore', 'Acharya_Institute_of_Technology | wasGivenTheTechnicalCampusStatusBy | All_India_Council_for_Technical_Education', 'Acharya_Institute_of_Technology | state | Karnataka']\". The model generated output is \"The Acharya Institute of Technology is located in Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090, Karnataka. Its director is Dr. G. P. Prabhukumar and it was founded by Kempe Gowda. The Institute is located in the city of Bangalore which was founded by Kempe Gowda.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 85.0, "DataCoverage": 79.66666666666667, "Fluency": 87.66666666666667, "Relevance": 87.33333333333333, "TextStructure": 86.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Director Ray Griggs, born in 1974, is the writer of the film Super Capers, starring Michael Rooker and Tom Sizemore.\". The input of model is \"['Super_Capers | writer | Ray_Griggs_(director)', 'Ray_Griggs_(director) | birthYear | 1974', 'Super_Capers | starring | Michael_Rooker', 'Super_Capers | starring | Tom_Sizemore']\". The model generated output is \"Ray Griggs is the director of Super Coaracy, which starred Tom Davutemore and starred Michael Rooker. Ray Griggs was born in 1974.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 56.0, "DataCoverage": 71.33333333333333, "Fluency": 65.0, "Relevance": 64.66666666666667, "TextStructure": 71.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"MotorSport Vision is located in the city of Fawkham.\". The input of model is \"MotorSport_Vision | city | Fawkham\". The model generated output is \"The Motor sport of Vision is in Fawkham.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.66666666666667, "DataCoverage": 93.33333333333333, "Fluency": 85.0, "Relevance": 96.0, "TextStructure": 99.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Punk blues and noise rock are genre of the album Turn Me On.\". The input of model is \"['Turn_Me_On_(album) | genre | Punk_blues', 'Turn_Me_On_(album) | genre | Noise_rock']\". The model generated output is \"Turn Me On performs Punk music and performs Noise rock music.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 41.666666666666664, "DataCoverage": 84.33333333333333, "Fluency": 63.666666666666664, "Relevance": 86.33333333333333, "TextStructure": 76.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The language code (ISO 639-2) of the English language is eng.\". The input of model is \"English_language | iso6392Code | eng\". The model generated output is \"The English language has the iso6392code eng.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.66666666666667, "DataCoverage": 98.33333333333333, "Fluency": 95.33333333333333, "Relevance": 94.0, "TextStructure": 85.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Lady Anne Monson\u2019s nationality is English but currently resides in India, whose total area is 3,287,590 km^2. One of the leaders of India is Chief Justice.\". The input of model is \"['Lady_Anne_Monson | nationality | Kingdom_of_England', 'India | areaTotal | 3287590000000.0', 'Lady_Anne_Monson | residence | India', 'India | leaderTitle | Chief Justice']\". The model generated output is \"The leader of India is called the Chief Justice and it has a total area of 3287590000000.0. Their nationality is Kingdom of England and they reside in India.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 89.0, "DataCoverage": 89.66666666666667, "Fluency": 67.33333333333333, "Relevance": 98.66666666666667, "TextStructure": 58.333333333333336}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Aaron Turner was born in Massachusetts and is a post metal musician who plays electric guitar and played with the band Twilight.\". The input of model is \"['Aaron_Turner | genre | Post-metal', 'Aaron_Turner | origin | Massachusetts', 'Aaron_Turner | associatedBand/associatedMusicalArtist | Twilight_(band)', 'Aaron_Turner | instrument | Electric_guitar']\". The model generated output is \"Aaron Turner is a performer of the musical genre Post metal and plays electric guitar for Twilight. He is originally from Massachusetts.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.66666666666667, "DataCoverage": 99.33333333333333, "Fluency": 96.33333333333333, "Relevance": 99.33333333333333, "TextStructure": 97.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Olga Bondareva, who was born on April 27, 1937, graduated from Leningrad State University.\". The input of model is \"['Olga_Bondareva | birthDate | 1937-04-27', 'Olga_Bondareva | almaMater | Leningrad_State_University']\". The model generated output is \"Leningrad State University is the alma mater of bhajji pedas, who was born on 27 July 1937.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 58.0, "DataCoverage": 92.33333333333333, "Fluency": 88.33333333333333, "Relevance": 89.33333333333333, "TextStructure": 92.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Brandon Carter was born on January 1, 1942 and worked professionally in the field of General Relativity. Brandon Carter was known for the Carter Constant and his contributions to the no-hair theorem. Dennis William Sciama was his doctoral advisor.\". The input of model is \"['Brandon_Carter | professionalField | General_relativity', 'Brandon_Carter | knownFor | Carter_constant', 'Brandon_Carter | knownFor | No-hair_theorem', 'Brandon_Carter | doctoralAdvisor | Dennis_William_Sciama', 'Brandon_Carter | birthDate | 1942-01-01']\". The model generated output is \"Brandon Carter was born in 1942-01-01 and is known as a carter for No-hair theorem. He was a professional professional field and a doctoral advisor for Dennis William Sciama. He retired in 1942-01-01.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 64.33333333333333, "DataCoverage": 94.33333333333333, "Fluency": 60.333333333333336, "Relevance": 89.33333333333333, "TextStructure": 88.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The location of Bedford Aerodrome is Thurleigh and it has a runway whose length is 1095.\". The input of model is \"['Bedford_Aerodrome | runwayLength | 1095.0', 'Bedford_Aerodrome | location | Thurleigh']\". The model generated output is \"The runway length of bakewell (located in Thurleigh) is 1095.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 75.33333333333333, "DataCoverage": 65.66666666666667, "Fluency": 52.333333333333336, "Relevance": 73.33333333333333, "TextStructure": 86.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Year of No Light is both the artist and the producer of the album Nord. The album was released on the label Crucial Blast. Year of No Light's next album was Live at Roadburn 2008.\". The input of model is \"['Nord_(Year_of_No_Light_album) | followedBy | Live_at_Roadburn_2008_(Year_of_No_Light_album)', 'Nord_(Year_of_No_Light_album) | producer | Year_of_No_Light', 'Nord_(Year_of_No_Light_album) | artist | Year_of_No_Light', 'Nord_(Year_of_No_Light_album) | recordLabel | Crucial_Blast']\". The model generated output is \"Located in the year of 27 December 2008, and produced by Live at Roadburn, and with a recording label called Crucial Blast, The asteroid, called bhangra Light, is signed to the record label Crucial Blast.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 69.0, "DataCoverage": 84.0, "Fluency": 64.5, "Relevance": 71.0, "TextStructure": 81.5}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Piotr Hallmann weighs 70.308 kilograms and was born in Gdynia, Poland, which follows the Central European Summer Time timezone.\". The input of model is \"['Piotr_Hallmann | birthPlace | Gdynia,_Poland', 'Piotr_Hallmann | weight | 70.308', 'Gdynia,_Poland | timeZone | Central_European_Time', 'Gdynia,_Poland | timeZone | Central_European_Summer_Time']\". The model generated output is \"Gdy-2, who weighs 70.308, was born in Gdy253, Poland. Gdy-speed is in the Central European Time zone and spent a total of 70.308 metres in space.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 32.0, "DataCoverage": 31.666666666666668, "Fluency": 11.0, "Relevance": 21.0, "TextStructure": 29.333333333333332}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The All India Council for Technical Education located in Mumbai was proud to give the Technical Campus status to the Acharya Institute of Technology. The Acharya Institute of Technology is affiliated with Visvesvaraya Technological University which is located at Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore - 560090.\". The input of model is \"['Acharya_Institute_of_Technology | campus | \"In Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090.\"', 'All_India_Council_for_Technical_Education | location | Mumbai', 'Acharya_Institute_of_Technology | affiliation | Visvesvaraya_Technological_University', 'Acharya_Institute_of_Technology | city | Bangalore', 'Acharya_Institute_of_Technology | wasGivenTheTechnicalCampusStatusBy | All_India_Council_for_Technical_Education']\". The model generated output is \"The Acharya Institute of Technology is located in Soldevanahalli, Acharya Dr. Sarvapalli Radhakrishnan Road, Hessarghatta Main Road, Bangalore \u2013 560090. It was given the technical campus status by the All India Council for Technical Education in Mumbai. The Institute is affiliated with the Visvesvaraya Technological University.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 86.0, "DataCoverage": 82.0, "Fluency": 87.33333333333333, "Relevance": 92.66666666666667, "TextStructure": 90.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Liselotte Grschebina was born 115 meters above sea-level, in Karlsruhe, in the German Empire, where the currency is the German Papiermark.\". The input of model is \"['Liselotte_Grschebina | birthPlace | German_Empire', 'German_Empire | currency | German_Papiermark', 'Liselotte_Grschebina | birthPlace | Karlsruhe', 'Karlsruhe | elevationAboveTheSeaLevel | 115.0']\". The model generated output is \"The German Empire uses the German Papiermark as its currency and is the birthplace of Karlsruhe. It is 115 metres above sea level and the German Empire is the birthplace of bhajji.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 68.33333333333333, "DataCoverage": 66.0, "Fluency": 54.333333333333336, "Relevance": 74.66666666666667, "TextStructure": 62.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The leader title for Ciudad Ayala is City Manager.\". The input of model is \"Ciudad_Ayala | leaderTitle | \"City Manager\"\". The model generated output is \"The leader of bakewell is the City Manager.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 41.0, "DataCoverage": 64.0, "Fluency": 58.666666666666664, "Relevance": 71.33333333333333, "TextStructure": 80.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The University of Burgundy, located in the city of Dijon (postal code 21000), has 16,800 undergraduate students and 2,900 staff members.\". The input of model is \"['University_of_Burgundy | city | Dijon', 'Dijon | postalCode | 21000', 'University_of_Burgundy | numberOfUndergraduateStudents | 16800', 'University_of_Burgundy | staff | 2900']\". The model generated output is \"The University of Burgundy in Dijon has a staff compliment of 2900 and 16800 students. Its postal code is 21000.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 94.66666666666667, "Relevance": 100.0, "TextStructure": 95.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Detroit is part of Michigan.\". The input of model is \"Detroit | isPartOf | Michigan\". The model generated output is \"Detroit is part of Michigan.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 87.33333333333333, "DataCoverage": 69.0, "Fluency": 87.0, "Relevance": 86.33333333333333, "TextStructure": 92.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Brian Kelly produced an instrumental album titled Expect A Miracle.\". The input of model is \"['Expect_a_Miracle | producer | Brian_Kelly_(composer_&_pianist)', 'Expect_a_Miracle | genre | Instrumental_music']\". The model generated output is \"Brian Kelly is a composer & pianist who produces the Instrumental genre of expect a Harrietstown.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 71.33333333333333, "DataCoverage": 96.0, "Fluency": 65.0, "Relevance": 73.66666666666667, "TextStructure": 70.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"\"Imagine\" is a song by musical artist, John Lennon.\". The input of model is \"Imagine_(John_Lennon_song) | musicalArtist | John_Lennon\". The model generated output is \"John Lennon is a musical artist for Imagine John Lennon.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 55.666666666666664, "DataCoverage": 65.66666666666667, "Fluency": 23.0, "Relevance": 88.33333333333333, "TextStructure": 36.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The album Turn Me On is punk blues, a genre which originated from the earlier garage rock sound.\". The input of model is \"['Turn_Me_On_(album) | genre | Punk_blues', 'Punk_blues | stylisticOrigin | Garage_rock']\". The model generated output is \"The musical genre of Turn Me On is Punk which originated from garage rock music.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 92.66666666666667, "DataCoverage": 81.0, "Fluency": 84.0, "Relevance": 98.66666666666667, "TextStructure": 91.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Train followed Mermaid with Imagine, a song by John Lennon was produced by the production team Espionage and written by Pat Monahan.\". The input of model is \"['Mermaid_(Train_song) | followedBy | Imagine_(John_Lennon_song)', 'Mermaid_(Train_song) | producer | Espionage_(production_team)', 'Mermaid_(Train_song) | writer | Pat_Monahan']\". The model generated output is \"Pat Monahan is the writer of bakso which is made by Espionage. It is followed by Imagine John Lennon Davutoglu and Pat Monahan.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 49.666666666666664, "DataCoverage": 64.0, "Fluency": 65.0, "Relevance": 51.333333333333336, "TextStructure": 79.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Aleksandr Prudnikov was born on February 24, 1989 and is 185 cm tall. He currently plays for FC Amkar Perm and also played for FC Terek Grozny, which is located in Grozny.\". The input of model is \"['FC_Terek_Grozny | ground | Grozny', 'Aleksandr_Prudnikov | currentclub | FC_Amkar_Perm', 'Aleksandr_Prudnikov | club | FC_Terek_Grozny', 'Aleksandr_Prudnikov | birthDate | 1989-02-24', 'Aleksandr_Prudnikov | height | 185.0 (centimetres)']\". The model generated output is \"Aleksandr Prudnikov was born on 24 February 1989 and is 185 cm tall. He currently plays for FC Amkar Perm and FC Terek Grozny, the latter of which is based in Grozny.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 89.33333333333333, "DataCoverage": 90.66666666666667, "Fluency": 95.66666666666667, "Relevance": 90.33333333333333, "TextStructure": 92.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Louis Levy, who was born on November 20, 1894, is the music composer for the film \"It's Great to be Young.\"\". The input of model is \"[\"It's_Great_to_Be_Young_(1956_film) | musicComposer | Louis_Levy\", 'Louis_Levy | birthDate | 1894-11-20']\". The model generated output is \"Louis Levy, born on 1894-11-20, is a composer of Louis Levy, who retired on 1 August 1969.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 83.33333333333333, "DataCoverage": 84.0, "Fluency": 73.66666666666667, "Relevance": 74.33333333333333, "TextStructure": 86.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"GMA New Media is a company in the mass media industry whose headquarters are located in the GMA Network Center.\". The input of model is \"['GMA_New_Media | industry | Mass_Media', 'GMA_New_Media | location | GMA_Network_Center']\". The model generated output is \"GMA New Media is a mass media industry located in the GMA Network Center.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 94.0, "Relevance": 100.0, "TextStructure": 100.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Hypermarcas, whose main product is drugs, was founded on January 1, 2001 and is located in Brazil.\". The input of model is \"['Hypermarcas | location | Brazil', 'Hypermarcas | product | Drugs', 'Hypermarcas | foundingDate | 2001-01-01']\". The model generated output is \"Hypermarcas, founded on January 1, 2001, is located in Brazil and makes drugs.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 96.33333333333333, "Fluency": 91.0, "Relevance": 96.66666666666667, "TextStructure": 98.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The place served by the Bedford Aerodrome located in Thurleigh is the Bedford Autodrome.\". The input of model is \"['Bedford_Aerodrome | cityServed | Bedford_Autodrome', 'Bedford_Aerodrome | location | Thurleigh']\". The model generated output is \"The location of bakewell pedas is Thurleigh and it serves the city of Autodrome.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 58.333333333333336, "DataCoverage": 72.66666666666667, "Fluency": 96.0, "Relevance": 73.33333333333333, "TextStructure": 90.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Ciudad Ayala city, led by Juan Nolasco, has a population density of 1604.0.\". The input of model is \"['Ciudad_Ayala | populationDensity | 1604.0', 'Ciudad_Ayala | leader | Juan_Nolasco']\". The model generated output is \"The population density of bakewell (led by Juan Nolasco) is 1604.0.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 56.666666666666664, "DataCoverage": 63.666666666666664, "Fluency": 75.33333333333333, "Relevance": 59.333333333333336, "TextStructure": 72.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The College of WIlliam and Mary is the current owner of Alan B. Miller Hall and the Mason School of Business is the current tenants.\". The input of model is \"['Alan_B._Miller_Hall | currentTenants | Mason_School_of_Business', 'Alan_B._Miller_Hall | owner | College_of_William_&_Mary']\". The model generated output is \"The Mason School of Business are the current tenants of Alan B. Miller Hall which is owned by The College of William and Mary.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 87.0, "DataCoverage": 98.33333333333333, "Fluency": 84.33333333333333, "Relevance": 99.66666666666667, "TextStructure": 84.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Justin Whalin stars in Super Capers, which is written by director Ray Griggs.\". The input of model is \"['Super_Capers | starring | Justin_Whalin', 'Super_Capers | writer | Ray_Griggs_(director)']\". The model generated output is \"Ray Griggs is the Dean of Super Coaracy which starred Justin Whalin.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 76.33333333333333, "DataCoverage": 68.66666666666667, "Fluency": 78.66666666666667, "Relevance": 67.33333333333333, "TextStructure": 86.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"To the northeast of Frederick County, Maryland is Adams County, Pennsylvania, the location of the 11th Mississippi Infantry Monument. The monument is a Contributing Property.\". The input of model is \"['11th_Mississippi_Infantry_Monument | location | Adams_County,_Pennsylvania', 'Adams_County,_Pennsylvania | hasToItsSouthwest | Frederick_County,_Maryland', '11th_Mississippi_Infantry_Monument | category | Contributing_property']\". The model generated output is \"The 11th Mississippi Infantry Monument falls under the category of Contributing property and is located in Adams County, Pennsylvania. To the southwest of Adams County lies Frederick County, Maryland.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 100.0, "DataCoverage": 100.0, "Fluency": 91.66666666666667, "Relevance": 100.0, "TextStructure": 92.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Pontiac Rageous was a car with a coupe body style manufactured by Pontiac. Assembled in both Michigan and Detroit, it went into production in 1997, ending in the same year.\". The input of model is \"['Pontiac_Rageous | productionStartYear | 1997', 'Pontiac_Rageous | assembly | Michigan', 'Pontiac_Rageous | assembly | Detroit', 'Pontiac_Rageous | productionEndYear | 1997', 'Pontiac_Rageous | bodyStyle | Coupe', 'Pontiac_Rageous | manufacturer | Pontiac']\". The model generated output is \"Pontiac Rageous, made in Pontiac, Michigan in 1997, is a Coupe assembled in Detroit. Its production start date is 1997.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 87.66666666666667, "DataCoverage": 89.66666666666667, "Fluency": 75.66666666666667, "Relevance": 95.33333333333333, "TextStructure": 95.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Chinabank was founded on August 16,1920 and the bank has 295 centers and provides services to customers. Chinabank's net income is 15,100,000,000.\". The input of model is \"['Chinabank | service | Banking', 'Chinabank | numberOfLocations | 295', 'Chinabank | foundingDate | 1920-08-16', 'Chinabank | netIncome | 15100000000']\". The model generated output is \"Chinabank, a banking service founded on 08-16-1920, has 295 locations and a net income of 15,100,000,000.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.66666666666667, "DataCoverage": 98.33333333333333, "Fluency": 99.0, "Relevance": 97.66666666666667, "TextStructure": 96.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"George Allen & Unwin published the Fellowship of the Ring.\". The input of model is \"The_Fellowship_of_the_Ring | publisher | George_Allen_&_Unwin\". The model generated output is \"George Allen & Unwin is the publisher of The Fellowship of the Ring.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 97.33333333333333, "DataCoverage": 98.66666666666667, "Fluency": 97.66666666666667, "Relevance": 97.66666666666667, "TextStructure": 91.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Alan Shepard, an American, who went to school at NWC and graduated with an MA in 1957 and who was the Chief of the Astronaut Office in 1963, was born in New Hampshire on Nov 18, 1923 and died in California.\". The input of model is \"['Alan_Shepard | birthDate | \"1923-11-18\"', 'Alan_Shepard | nationality | United_States', 'Alan_Shepard | servedAsChiefOfTheAstronautOfficeIn | 1963', 'Alan_Shepard | deathPlace | California', 'Alan_Shepard | birthPlace | New_Hampshire', 'Alan_Shepard | almaMater | \"NWC, M.A. 1957\"', 'Alan_Shepard | status | \"Deceased\"']\". The model generated output is \"Alan Shepard was an American born in New Hampshire on November 18th, 1923. He graduated from NWC with an M.A. in 1957 and died in California. He served as chief of the astronaut office in 1963.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.66666666666667, "DataCoverage": 96.66666666666667, "Fluency": 98.0, "Relevance": 94.33333333333333, "TextStructure": 95.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"English Without Tears is 89 minutes long.\". The input of model is \"English_Without_Tears | runtime | 89.0\". The model generated output is \"The English Airbase has an apoapsis of 89.0.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 38.0, "DataCoverage": 38.0, "Fluency": 93.33333333333333, "Relevance": 38.333333333333336, "TextStructure": 95.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Year of No Light is a band whose members include a former guitarist for Monarch has created an album called Nord.\". The input of model is \"['Nord_(Year_of_No_Light_album) | artist | Year_of_No_Light', 'Year_of_No_Light | associatedBand/associatedMusicalArtist | Monarch_(band)']\". The model generated output is \"The musical artist of the band bhangra Light is Monarch and they were once a member of the band NRBQ.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 43.333333333333336, "DataCoverage": 28.666666666666668, "Fluency": 42.0, "Relevance": 27.0, "TextStructure": 44.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Musical group the Velvet Underground released The album The Bootleg Series Volume 1: The Quine Tapes, which was recorded under the Universal Music Group and its parent company Vivendi. A former member of the band was Lou Reed.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | recordLabel | Universal_Music_Group', 'Universal_Music_Group | parentCompany | Vivendi', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | artist | The_Velvet_Underground', 'The_Velvet_Underground | formerBandMember | Lou_Reed']\". The model generated output is \"Lou Reed is a member of The Velvet Underground which was formerly a member of The Vivendi based Universal Music Group. The record label of the asteroid, called The Velvet Underground, is called The Vazendi and the record label is Universal Music Group.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 92.5, "DataCoverage": 92.5, "Fluency": 92.5, "Relevance": 98.0, "TextStructure": 96.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The band Train were signed to the record label Warner Music Group.\". The input of model is \"Train_(band) | recordLabel | Warner_Music_Group\". The model generated output is \"The record label of 1089 (band) is Warner Music Group.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 58.333333333333336, "DataCoverage": 34.666666666666664, "Fluency": 53.0, "Relevance": 35.666666666666664, "TextStructure": 62.333333333333336}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Honeymoon Killers is the producer of the Turn Me On album.\". The input of model is \"Turn_Me_On_(album) | producer | The_Honeymoon_Killers_(American_band)\". The model generated output is \"Turn Me On was manufactured by The Honeoon Killers, an American band.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 92.66666666666667, "DataCoverage": 59.0, "Fluency": 79.66666666666667, "Relevance": 59.333333333333336, "TextStructure": 98.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Agremiacao Sportiva Arapiraquense play in the Campeonato Brasileiro Serie C league based in Brazil in 2015. Their ground is the Estadio Municipal Coaracy da Mata Fonseca located in Arapiraca.\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | season | 2015', 'Campeonato_Brasileiro_S\u00e9rie_C | country | Brazil', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca']\". The model generated output is \"Agremiacao Sportiva Arapiraquense's ground is the Estadio Municipal Coaracy da Mata Fonseca in Arapiraca, Brazil. They competed in the 2015 season in the Campeonato Brasileiro S\u00e9rie C league and play their home games at Est\u00e1dio Municipal Coaracy da Mata Fonseca.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 81.0, "DataCoverage": 94.33333333333333, "Fluency": 69.0, "Relevance": 92.66666666666667, "TextStructure": 80.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nurhan Atasoy was born on January 1st 1934 in Turkey.\". The input of model is \"['Nurhan_Atasoy | birthDate | 1934-01-01', 'Nurhan_Atasoy | birthPlace | Turkey']\". The model generated output is \"NurB\u00fct\u00fcner Atasoy was born in Turkey on 1934-01-01.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 67.33333333333333, "DataCoverage": 98.66666666666667, "Fluency": 95.66666666666667, "Relevance": 97.33333333333333, "TextStructure": 97.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Brandon Carter, a specialist in General Relativity, is known for the Carter Constant.\". The input of model is \"['Brandon_Carter | professionalField | General_relativity', 'Brandon_Carter | knownFor | Carter_constant']\". The model generated output is \"Brandon Carter, known as a carter, professional field is General relativity, is known as a carter.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 54.666666666666664, "DataCoverage": 39.666666666666664, "Fluency": 34.333333333333336, "Relevance": 46.333333333333336, "TextStructure": 48.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The population in the Ciudad Ayala metropolitan area is 1777539.\". The input of model is \"Ciudad_Ayala | populationMetro | 1777539\". The model generated output is \"The population metro of bakewell is 1777539.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 13.666666666666666, "DataCoverage": 72.66666666666667, "Fluency": 82.33333333333333, "Relevance": 70.66666666666667, "TextStructure": 82.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The BBC tv series, Bananaman, created by Steve Bright and starred Bill Oddie, who was born in Rochdale. The BBC Headquarters is referred to as Broadcasting House.\". The input of model is \"['BBC | city | Broadcasting_House', 'Bananaman | starring | Bill_Oddie', 'Bananaman | creator | Steve_Bright', 'Bill_Oddie | birthPlace | Rochdale', 'Bananaman | broadcastedBy | BBC']\". The model generated output is \"Bananaman was created by Steve Bright and starred Bill Oddie who was born in Rochdale. It was broadcast by the BBC which is based in Broadcasting House in London.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.66666666666667, "DataCoverage": 91.33333333333333, "Fluency": 87.66666666666667, "Relevance": 91.0, "TextStructure": 91.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Expect a Miracle is in the instrumental music genre.\". The input of model is \"Expect_a_Miracle | genre | Instrumental_music\". The model generated output is \"Instrumental music is the genre of the expecta Davuto\u011flu acle.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 54.0, "DataCoverage": 44.0, "Fluency": 70.33333333333333, "Relevance": 57.0, "TextStructure": 85.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Bionico is a dessert food found in Mexico where Enrique Pena Nieto is the leader.\". The input of model is \"['Bionico | course | Dessert', 'Bionico | country | Mexico', 'Mexico | leader | Enrique_Pe\u00f1a_Nieto']\". The model generated output is \"Bionico is a dessert found in Mexico where Enrique Pena Nieto is a leader.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 87.0, "DataCoverage": 98.0, "Fluency": 85.0, "Relevance": 95.66666666666667, "TextStructure": 85.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Bionico is a dessert found in Mexico where peso is the currency. Cookie is also a dessert.\". The input of model is \"['Mexico | currency | Mexican_peso', 'Bionico | course | Dessert', 'Dessert | dishVariation | Cookie', 'Bionico | country | Mexico']\". The model generated output is \"Bionico is a dessert found in Mexico where the currency is the peso. Cookies are also a type of dessert.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 92.0, "DataCoverage": 72.66666666666667, "Fluency": 90.66666666666667, "Relevance": 92.66666666666667, "TextStructure": 91.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Abraham A. Ribicoff\u2019s birthplace is the USA where he worked for the United States Secretary of Health, Education, and Welfare office, in a country where African Americans are a national ethnic group. His spouse is Ruth Ribicoff.\". The input of model is \"['Abraham_A._Ribicoff | spouse | \"Ruth Ribicoff\"', 'Abraham_A._Ribicoff | birthPlace | United_States', 'Abraham_A._Ribicoff | office | \"United States Secretary of Health, Education, and Welfare\"', 'United_States | ethnicGroup | African_Americans']\". The model generated output is \"African Americans are an ethnic group in the United States where Abraham A. Ribicoff was born. He was married to Ruth Ribicoff and worked as the United States Secretary of Health, Education, and Welfare.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 93.66666666666667, "DataCoverage": 92.66666666666667, "Fluency": 92.0, "Relevance": 97.0, "TextStructure": 97.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Andrew Rayel who performed the genre of Trance music is associated with the musical artist Armin van Buuren.\". The input of model is \"['Andrew_Rayel | genre | Trance_music', 'Andrew_Rayel | associatedBand/associatedMusicalArtist | Armin_van_Buuren']\". The model generated output is \"Andrew Rayel plays Trance music and is associated with Armin van Buuren.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 98.0, "DataCoverage": 99.33333333333333, "Fluency": 95.0, "Relevance": 99.66666666666667, "TextStructure": 94.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Year of No Light album Nord is in the sludge metal genre, one variety of which is southern sludge.\". The input of model is \"['Nord_(Year_of_No_Light_album) | genre | Sludge_metal', 'Sludge_metal | musicSubgenre | Southern_sludge']\". The model generated output is \"Sludge metal is a sub genre of Southern sludge and is the genre of the player, called the band bhangra.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 74.33333333333333, "DataCoverage": 77.33333333333333, "Fluency": 74.0, "Relevance": 80.0, "TextStructure": 69.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Jamie Lawrence is the music composer of the 83 minute 'Death on a Factory Farm' film, directed by Sarah Teale.\". The input of model is \"['Death_on_a_Factory_Farm | musicComposer | Jamie_Lawrence', 'Death_on_a_Factory_Farm | runtime | 83.0', 'Death_on_a_Factory_Farm | director | Sarah_Teale']\". The model generated output is \"Jamie Lawrence is the composer of the music composer of Death on pedas, 83.0 m long and the director is Sarah Teale.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 69.66666666666667, "DataCoverage": 84.33333333333333, "Fluency": 61.0, "Relevance": 85.66666666666667, "TextStructure": 61.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"MotorSport Vision, founded in 2004 by Jonathan Palmer, operates the Bedford Aerodrome in Thurleigh.\". The input of model is \"['Bedford_Aerodrome | operatingOrganisation | MotorSport_Vision', 'MotorSport_Vision | foundedBy | Jonathan_Palmer', 'MotorSport_Vision | foundingYear | 2004', 'Bedford_Aerodrome | location | Thurleigh']\". The model generated output is \"The Motor sport Vision, founded by Jonathan Palmer in 2004, is operated by the Motor sport Vision located in Thurleigh.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 69.0, "DataCoverage": 89.33333333333333, "Fluency": 84.33333333333333, "Relevance": 99.66666666666667, "TextStructure": 84.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"1634: The Bavarian Crisis which has 448 pages, was written by Eric Flint.\". The input of model is \"['1634:_The_Bavarian_Crisis | numberOfPages | \"448\"', '1634:_The_Bavarian_Crisis | author | Eric_Flint']\". The model generated output is \"Eric Flint is the author of 1634: The Bavarian Crisis which has 448 pages.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.33333333333333, "DataCoverage": 85.0, "Fluency": 88.66666666666667, "Relevance": 79.0, "TextStructure": 87.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Ciudad Ayala UTC offset is -6 in the country of Mexico.\". The input of model is \"['Ciudad_Ayala | utcOffset | \u22126', 'Ciudad_Ayala | country | Mexico']\". The model generated output is \"The UTC offset for bakewell pedas, a food found in Mexico, is Sumitra 6.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 40.0, "DataCoverage": 47.0, "Fluency": 53.0, "Relevance": 35.0, "TextStructure": 59.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The total area of water in New Jersey is 3,544 square kilometers.\". The input of model is \"New_Jersey | areaOfWater | 3544040000.0\". The model generated output is \"3544040000.0 is the area of water in New Jersey.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.66666666666667, "DataCoverage": 94.0, "Fluency": 82.66666666666667, "Relevance": 92.66666666666667, "TextStructure": 91.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Expect a Miracle has a runtime of 54:56.\". The input of model is \"Expect_a_Miracle | runtime | 54.56\". The model generated output is \"The acle's apoapsis is 54.56.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 46.333333333333336, "DataCoverage": 52.333333333333336, "Fluency": 96.66666666666667, "Relevance": 53.333333333333336, "TextStructure": 86.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Train followed Mermaid with Imagine is a song by musical artist John Lennon, which has a FIMI certification. It was produced by the production team Espionage.\". The input of model is \"['Mermaid_(Train_song) | followedBy | Imagine_(John_Lennon_song)', 'Imagine_(John_Lennon_song) | certification | FIMI', 'Mermaid_(Train_song) | producer | Espionage_(production_team)', 'Imagine_(John_Lennon_song) | musicalArtist | John_Lennon']\". The model generated output is \"The certification FIMI certification is FIMI, and it is followed by an impressive certification. John Lennon made the Espionage production team and is the musical artist John Lennon.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 76.0, "DataCoverage": 94.0, "Fluency": 76.33333333333333, "Relevance": 76.66666666666667, "TextStructure": 70.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"GMA New Media is a company in the entertainment industry that is located inside the GMA Network Center, which is in the Philippines,.\". The input of model is \"['GMA_New_Media | location | GMA_Network_Center', 'GMA_New_Media | location | Philippines', 'GMA_New_Media | industry | Entertainment']\". The model generated output is \"GMA New Media, located in the GMA Network Center in the Philippines, is an entertainment industry.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 91.66666666666667, "DataCoverage": 99.66666666666667, "Fluency": 81.0, "Relevance": 96.66666666666667, "TextStructure": 81.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Piotr Hallmann was born on August 25, 1987 and he weighs 70.308 kilograms.\". The input of model is \"['Piotr_Hallmann | weight | 70.308', 'Piotr_Hallmann | birthDate | 1987-08-25']\". The model generated output is \"piotr Hall-2, who weighs 70.308, retired 05 / 05 / 1983.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 53.0, "DataCoverage": 55.666666666666664, "Fluency": 20.666666666666668, "Relevance": 54.0, "TextStructure": 34.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Nie Haisheng is from the People's Republic of China, he was born in Zaoyang, Hubei.\". The input of model is \"['Nie_Haisheng | birthPlace | Hubei', \"Nie_Haisheng | nationality | People's_Republic_of_China\", 'Nie_Haisheng | birthPlace | Zaoyang']\". The model generated output is \"Nie Haisheng was born in Hubei, Zaoyang, People's Republic of China.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 84.66666666666667, "DataCoverage": 94.33333333333333, "Fluency": 96.66666666666667, "Relevance": 100.0, "TextStructure": 100.0}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"ENAIRE, located in Madrid, is the operating organisation for Adolfo Suarez Madrid-Barajas airport in Alcobendas. The airport is elevated 610 metres above sea level and its runway name is 14L/32R.\". The input of model is \"['Adolfo_Su\u00e1rez_Madrid\u2013Barajas_Airport | operatingOrganisation | ENAIRE', 'Adolfo_Su\u00e1rez_Madrid\u2013Barajas_Airport | location | Alcobendas', 'ENAIRE | city | Madrid', 'Adolfo_Su\u00e1rez_Madrid\u2013Barajas_Airport | elevationAboveTheSeaLevel | 610.0', 'Adolfo_Su\u00e1rez_Madrid\u2013Barajas_Airport | runwayName | \"14L/32R\"']\". The model generated output is \"Adolfo Su\u00e1rez Madrid \u2013 Barajas Airport is located in Alcobendas and operated by ENAIRE. The airport is 610 metres above sea level and has the runway name 14L / 32R.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 91.33333333333333, "DataCoverage": 86.66666666666667, "Fluency": 76.66666666666667, "Relevance": 81.66666666666667, "TextStructure": 89.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The Mason School of Business are the current tenants of Alan B Miller Hall, which was completed on June 1, 2009.\". The input of model is \"['Alan_B._Miller_Hall | currentTenants | Mason_School_of_Business', 'Alan_B._Miller_Hall | completionDate | 2009-06-01']\". The model generated output is \"The Mason School of Business are the current tenants of Alan B. Miller Hall which was completed on June 1st, 2009.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 95.66666666666667, "DataCoverage": 99.0, "Fluency": 92.0, "Relevance": 95.33333333333333, "TextStructure": 95.66666666666667}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Piotr Hallmann's birthplace was in Gdynia, Poland on August 25th. 1987 and he is 175.26 cm tall.\". The input of model is \"['Piotr_Hallmann | height | 175.26', 'Piotr_Hallmann | birthPlace | Gdynia,_Poland', 'Piotr_Hallmann | birthDate | 1987-08-25']\". The model generated output is \"The birth place of piotr Hall-2is Gdy-2, Poland. It is 175.26 metres tall and retired 05 / 05 / 1987.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 42.0, "DataCoverage": 38.333333333333336, "Fluency": 56.666666666666664, "Relevance": 38.666666666666664, "TextStructure": 73.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Bootleg Series Volume 1: The Quine Tapes released on October 16, 2001 is 230.05 minutes long.\". The input of model is \"['Bootleg_Series_Volume_1:_The_Quine_Tapes | releaseDate | 2001-10-16', 'Bootleg_Series_Volume_1:_The_Quine_Tapes | runtime | 230.05']\". The model generated output is \"The release date for 1089 -21 (The) is 2001-10-16 and it has an apoapsis of 230.05.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 60.333333333333336, "DataCoverage": 70.66666666666667, "Fluency": 96.33333333333333, "Relevance": 68.66666666666667, "TextStructure": 93.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"The album Expect a miracle is of the easy instrumental Listening music genre.\". The input of model is \"['Expect_a_Miracle | genre | Instrumental_music', 'Expect_a_Miracle | genre | Easy_listening']\". The model generated output is \"Instrumental music is the genre of the expecting genre, Easy listening, as well as being the genre of the expect a Sumitra.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 45.333333333333336, "DataCoverage": 63.333333333333336, "Fluency": 43.0, "Relevance": 68.66666666666667, "TextStructure": 55.666666666666664}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Agremia\u00e7\u00e3o Sportiva Arapiraquense with 17000 members play in the Brazil Campeonato Brasileiro S\u00e9rie C league. Est\u00e1dio Municipal Coaracy da Mata Fonseca in located in Arapiraca and ground is Agremia\u00e7\u00e3o Sportiva Arapiraquense.\". The input of model is \"['Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_S\u00e9rie_C', 'Campeonato_Brasileiro_S\u00e9rie_C | country | Brazil', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | numberOfMembers | 17000', 'Agremia\u00e7\u00e3o_Sportiva_Arapiraquense | ground | Est\u00e1dio_Municipal_Coaracy_da_Mata_Fonseca']\". The model generated output is \"Agremiacao Sportiva Arapiraquense's ground is the Estadio Municipal Coaracy da Mata Fonseca in Arapiraca, Brazil. They play in the Campeonato Brasileiro S\u00e9rie C league and have 17000 members.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 62.333333333333336, "DataCoverage": 93.66666666666667, "Fluency": 90.66666666666667, "Relevance": 95.0, "TextStructure": 83.33333333333333}}, {"input": "You are evaluating RDF-to-text task. The correct generation is \"Al Asad Airbase has a runway length of 3,992.88 and is run by the United States Air Force.\". The input of model is \"['Al_Asad_Airbase | operatingOrganisation | United_States_Air_Force', 'Al_Asad_Airbase | runwayLength | 3992.88']\". The model generated output is \"The United States Air Force is the operating organisation for Al Asad airbase which has a runway length of 3992.88.\n\". Please identify all errors within each model output, up to a maximum of five. For each error, please give me the corresponding error dimension, error type, major/minor label, error location of the model generated output and explanation for the error. Major errors can confuse or mislead the reader due to significant change in meaning, while minor errors don't lead to loss of meaning but will be noticed.", "output": {"Correctness": 99.66666666666667, "DataCoverage": 99.66666666666667, "Fluency": 99.33333333333333, "Relevance": 99.66666666666667, "TextStructure": 99.33333333333333}}]}